{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7362783",
   "metadata": {},
   "source": [
    "# Problem Definition\n",
    "Creating a model that will first find out wether a person is eligible for loan or not and if eligible than how much loan the user can obtain based on various factors such as the user’s income, education, etc.\n",
    "## Data Source\n",
    "https://www.kaggle.com/datasets/altruistdelhite04/loan-prediction-problem-dataset\n",
    "## Conditions that must match for this model\n",
    "First of all, one cannot predict loan amount using regression models and 981 instances (combining training and testing set) with R2 Score greater than 0.44\n",
    "\n",
    "Secondly, the dateset contains many missing values. So even if using imputer with different parameters for categorical and non-categorical data, ultimately data-set loses variability to some extent.\n",
    "\n",
    "You do imputation, label encoding, one-hot encoding and standardization, and even hyper-parameters tuning in an effort to increase accuracy. But the performance benchmarks around R2 Score of 0.44\n",
    "\n",
    "If still want to try to improve accuracy, you test different combinations of features. But still you get no significant improvement gains\n",
    "\n",
    "Leaving regressors aside now and testing classification to somehow improve accuracy by categorizing loan amount by defining ranges, you get to see accuracy jumping to ~0.7 and F1 Score to ~0.6 with all the optimization techniques applied above.\n",
    "\n",
    "It's tested and proven with all available ML models. So to better predict loan amount better increase the number of instances to more than 5000!\n",
    "\n",
    "These are the some conditions that must match in this model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "da857da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the required modules for the mcahine learning project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13085c0d",
   "metadata": {},
   "source": [
    "## Getting the Data ready for splitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da843533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we dont require to split data as it is already splitted for us \n",
    "#data = pd.read_excel(\"LoanPredData/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba30b62",
   "metadata": {},
   "source": [
    "## Accessing the data\n",
    "Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "912a5d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using readcsv as the data we have in exel is in the csv format\n",
    "train_set = pd.read_csv(\"LoanPredData/train_u6lujuX_CVtuZ9i.csv\")\n",
    "xtrain_set_df_features = train_set_df.drop(\"Loan_Status\",axis=1)\n",
    "ytrain_set_df_target = train_set[\"Loan_Status\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1cb119",
   "metadata": {},
   "source": [
    "## Accesing the data\n",
    "Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3f29fc96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loan_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Married</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Education</th>\n",
       "      <th>Self_Employed</th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>Property_Area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LP001015</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>5720</td>\n",
       "      <td>0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LP001022</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>3076</td>\n",
       "      <td>1500</td>\n",
       "      <td>126.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LP001031</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>5000</td>\n",
       "      <td>1800</td>\n",
       "      <td>208.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LP001035</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>2340</td>\n",
       "      <td>2546</td>\n",
       "      <td>100.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LP001051</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Not Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>3276</td>\n",
       "      <td>0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>LP002971</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3+</td>\n",
       "      <td>Not Graduate</td>\n",
       "      <td>Yes</td>\n",
       "      <td>4009</td>\n",
       "      <td>1777</td>\n",
       "      <td>113.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>LP002975</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>4158</td>\n",
       "      <td>709</td>\n",
       "      <td>115.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>LP002980</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>3250</td>\n",
       "      <td>1993</td>\n",
       "      <td>126.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Semiurban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>LP002986</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>5000</td>\n",
       "      <td>2393</td>\n",
       "      <td>158.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Rural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>LP002989</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Yes</td>\n",
       "      <td>9200</td>\n",
       "      <td>0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Rural</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>367 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Loan_ID Gender Married Dependents     Education Self_Employed  \\\n",
       "0    LP001015   Male     Yes          0      Graduate            No   \n",
       "1    LP001022   Male     Yes          1      Graduate            No   \n",
       "2    LP001031   Male     Yes          2      Graduate            No   \n",
       "3    LP001035   Male     Yes          2      Graduate            No   \n",
       "4    LP001051   Male      No          0  Not Graduate            No   \n",
       "..        ...    ...     ...        ...           ...           ...   \n",
       "362  LP002971   Male     Yes         3+  Not Graduate           Yes   \n",
       "363  LP002975   Male     Yes          0      Graduate            No   \n",
       "364  LP002980   Male      No          0      Graduate            No   \n",
       "365  LP002986   Male     Yes          0      Graduate            No   \n",
       "366  LP002989   Male      No          0      Graduate           Yes   \n",
       "\n",
       "     ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
       "0               5720                  0       110.0             360.0   \n",
       "1               3076               1500       126.0             360.0   \n",
       "2               5000               1800       208.0             360.0   \n",
       "3               2340               2546       100.0             360.0   \n",
       "4               3276                  0        78.0             360.0   \n",
       "..               ...                ...         ...               ...   \n",
       "362             4009               1777       113.0             360.0   \n",
       "363             4158                709       115.0             360.0   \n",
       "364             3250               1993       126.0             360.0   \n",
       "365             5000               2393       158.0             360.0   \n",
       "366             9200                  0        98.0             180.0   \n",
       "\n",
       "     Credit_History Property_Area  \n",
       "0               1.0         Urban  \n",
       "1               1.0         Urban  \n",
       "2               1.0         Urban  \n",
       "3               NaN         Urban  \n",
       "4               1.0         Urban  \n",
       "..              ...           ...  \n",
       "362             1.0         Urban  \n",
       "363             1.0         Urban  \n",
       "364             NaN     Semiurban  \n",
       "365             1.0         Rural  \n",
       "366             1.0         Rural  \n",
       "\n",
       "[367 rows x 12 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Set = pd.read_csv(\"LoanPredData/test_Y3wMUE5_7gLdaTN.csv\")\n",
    "xtest_Set_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "58acff4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(614, 367)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of the data we have\n",
    "len(train_set_df),len(xtest_Set_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ad8f52",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis( EDA)\n",
    "1) What are we trying to solve\n",
    "\n",
    "We are trying to create a ml model which will tell us how much loan the user can obtain using various attributes\n",
    "\n",
    "2) Type of Data we have\n",
    "\n",
    "We got the data both in numbers and strings also integers and strings mixed\n",
    "\n",
    "3) Missing Values\n",
    "\n",
    "We will be checking that int the code below\n",
    "\n",
    "4) What are the outliers(Random samples too different from other or not)\n",
    "\n",
    "Outliers are those data points that are significantly different from the rest of the dataset. They are often abnormal observations that skew the data distribution, and arise due to inconsistent data entry, or erroneous observations, will check that also\n",
    "\n",
    "5) Add change or remove features to get the most out of our data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c740221d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loan_ID               0\n",
       "Gender               13\n",
       "Married               3\n",
       "Dependents           15\n",
       "Education             0\n",
       "Self_Employed        32\n",
       "ApplicantIncome       0\n",
       "CoapplicantIncome     0\n",
       "LoanAmount           22\n",
       "Loan_Amount_Term     14\n",
       "Credit_History       50\n",
       "Property_Area         0\n",
       "Loan_Status           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for missing values and yes we have too many of them\n",
    "train_set.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5f251c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[str,\n",
       " str,\n",
       " str,\n",
       " str,\n",
       " str,\n",
       " str,\n",
       " numpy.int64,\n",
       " numpy.float64,\n",
       " numpy.float64,\n",
       " numpy.float64,\n",
       " numpy.float64,\n",
       " str,\n",
       " str]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for the data types of each columns\n",
    "type_testing_list = []\n",
    "for key,value in enumerate(train_set_df):\n",
    "    x = type(train_set_df[value][6])\n",
    "    type_testing_list.append(x)\n",
    "type_testing_list\n",
    "\n",
    "# # (other method)code for checking the length,dtype of each column\n",
    "# for key,values in train_set_df.iteritems():\n",
    "#     print(key,values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e667fc",
   "metadata": {},
   "source": [
    "### Now we will be checking wether the user is eligible for loan or not ,using classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8e6b631f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loan_Status\n",
       "Y    422\n",
       "N    192\n",
       "dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# first finding out how many of classes are there on our target variable\n",
    "loan_status = train_set_df.value_counts(\"Loan_Status\")\n",
    "loan_status.plot(kind=\"bar\",color=[\"Green\",\"Blue\"])\n",
    "loan_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9a9aff40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 614 entries, 0 to 613\n",
      "Data columns (total 13 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Loan_ID            614 non-null    object \n",
      " 1   Gender             601 non-null    object \n",
      " 2   Married            611 non-null    object \n",
      " 3   Dependents         599 non-null    object \n",
      " 4   Education          614 non-null    object \n",
      " 5   Self_Employed      582 non-null    object \n",
      " 6   ApplicantIncome    614 non-null    int64  \n",
      " 7   CoapplicantIncome  614 non-null    float64\n",
      " 8   LoanAmount         592 non-null    float64\n",
      " 9   Loan_Amount_Term   600 non-null    float64\n",
      " 10  Credit_History     564 non-null    float64\n",
      " 11  Property_Area      614 non-null    object \n",
      " 12  Loan_Status        614 non-null    object \n",
      "dtypes: float64(4), int64(1), object(8)\n",
      "memory usage: 62.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train_set_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3c44b30f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Credit_History</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>600.00000</td>\n",
       "      <td>564.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5403.459283</td>\n",
       "      <td>1621.245798</td>\n",
       "      <td>146.412162</td>\n",
       "      <td>342.00000</td>\n",
       "      <td>0.842199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6109.041673</td>\n",
       "      <td>2926.248369</td>\n",
       "      <td>85.587325</td>\n",
       "      <td>65.12041</td>\n",
       "      <td>0.364878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>12.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2877.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>360.00000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3812.500000</td>\n",
       "      <td>1188.500000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>360.00000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5795.000000</td>\n",
       "      <td>2297.250000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>360.00000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>81000.000000</td>\n",
       "      <td>41667.000000</td>\n",
       "      <td>700.000000</td>\n",
       "      <td>480.00000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
       "count       614.000000         614.000000  592.000000         600.00000   \n",
       "mean       5403.459283        1621.245798  146.412162         342.00000   \n",
       "std        6109.041673        2926.248369   85.587325          65.12041   \n",
       "min         150.000000           0.000000    9.000000          12.00000   \n",
       "25%        2877.500000           0.000000  100.000000         360.00000   \n",
       "50%        3812.500000        1188.500000  128.000000         360.00000   \n",
       "75%        5795.000000        2297.250000  168.000000         360.00000   \n",
       "max       81000.000000       41667.000000  700.000000         480.00000   \n",
       "\n",
       "       Credit_History  \n",
       "count      564.000000  \n",
       "mean         0.842199  \n",
       "std          0.364878  \n",
       "min          0.000000  \n",
       "25%          1.000000  \n",
       "50%          1.000000  \n",
       "75%          1.000000  \n",
       "max          1.000000  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e86f12",
   "metadata": {},
   "source": [
    "### Now we will be doing data study, like on what factors our model dependa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ac5bd01c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gender\n",
       "Male      489\n",
       "Female    112\n",
       "dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender = train_set_df.value_counts(\"Gender\")\n",
    "gender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9205c8a9",
   "metadata": {},
   "source": [
    "### comparing how gender affects the target variable loan status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "452564a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Loan_Status</th>\n",
       "      <th>N</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Female</th>\n",
       "      <td>37</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Male</th>\n",
       "      <td>150</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Loan_Status    N    Y\n",
       "Gender               \n",
       "Female        37   75\n",
       "Male         150  339"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.crosstab(train_set_df[\"Gender\"],train_set_df[\"Loan_Status\"])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "51c6ba10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Gender'>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAFMCAYAAADrxlayAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAtYklEQVR4nO3de1wU9d4H8M9yW7kuAsIuioiJvlTwhj5ejnlFFFPz8kpLKzCOZV5OHCALzaTLA2rl5eTJ8mjipR58Tkeso6ZiBqVkR0kT1DxeUFHZSIRdQFwIfs8fHedpBdSFxXWGz/v1mtfLnfnNzHdW/DD+5jczKiGEABERyYadrQsgIiLLMLiJiGSGwU1EJDMMbiIimWFwExHJDIObiEhmGNxERDLD4CYikhkGNxGRzDjYuoDGqK2txbVr1+Du7g6VSmXrcoiImkwIgbKyMvj7+8PO7h7n1MICH3zwgQgNDRXu7u7C3d1dDBgwQOzevVtaHhUVJQCYTf379zfbxq1bt8S8efOEt7e3cHFxEePHjxcFBQWWlCEKCgrq7IcTJ06clDDdTx5adMbdrl07LF26FJ06dQIAbNq0CY8//jiOHTuG7t27AwDGjBmDjRs3Sus4OTmZbSM2Nhb//Oc/kZaWBm9vb8THx2PcuHHIycmBvb39fdXh7u4OACgoKICHh4clh0BE9FAyGo0ICAiQ8u1uVEI07SFTXl5eeOeddxATE4Po6GiUlpZix44d9bY1GAxo06YNtmzZgmnTpgEArl27hoCAAOzevRujR4++r30ajUZoNBoYDAYGNxEpgiW51uiLkzU1NUhLS0NFRQUGDhwozc/MzISvry86d+6MWbNmoaioSFqWk5OD6upqRERESPP8/f0REhKC7OzsBvdlMplgNBrNJiKilsri4M7NzYWbmxvUajVmz56N9PR0dOvWDQAQGRmJTz75BAcOHMB7772HI0eOYMSIETCZTAAAvV4PJycntG7d2mybfn5+0Ov1De4zJSUFGo1GmgICAiwtm4hIMSweVdKlSxccP34cpaWl+Mc//oGoqChkZWWhW7duUvcHAISEhKBv374IDAzErl27MHny5Aa3KYS46+iQxMRExMXFSZ9v9wUREbVEFge3k5OTdHGyb9++OHLkCFavXo2PPvqoTludTofAwECcPXsWAKDValFVVYWSkhKzs+6ioiIMGjSowX2q1Wqo1WpLS0VNTQ2qq6stXo8ax9HR8b4vMBNR4zV5HLcQQuoKuVNxcTEKCgqg0+kAAGFhYXB0dERGRgamTp0KACgsLEReXh6WL1/e1FLMatLr9SgtLbXaNun+eHp6QqvVcnw9UTOyKLgXLlyIyMhIBAQEoKysDGlpacjMzMSePXtQXl6OpKQkTJkyBTqdDhcvXsTChQvh4+ODSZMmAQA0Gg1iYmIQHx8Pb29veHl5ISEhAaGhoQgPD7faQd0ObV9fX7i4uDBEHgAhBG7evCldjL79y5qIrM+i4P7555/xzDPPoLCwEBqNBj169MCePXswatQoVFZWIjc3F5s3b0ZpaSl0Oh2GDx+Obdu2mY1LXLlyJRwcHDB16lRUVlZi5MiRSE1Ntdp/sWtqaqTQ9vb2tso26f44OzsD+K3ry9fXl90mRM2kyeO4beFu4x1v3bqF/Px8dOjQQQoSenAqKytx8eJFBAUFoVWrVrYuh0g2LBnHLctnldwPdo/YBr93uu1h/lGQ3+mqOT4dkIhIZhjcREQy0+KCOzo6GhMnTrR1GQ366KOP0LNnT7i6usLT0xO9e/fGsmXLpOWNrT8pKQm9evWyXqFEZDOK7eOWow0bNiAuLg5/+ctfMHToUJhMJpw4cQKnTp2ydWlE9BBpcWfcd5OVlYX/+q//glqthk6nw6uvvopff/1VWr5nzx4MHjwYnp6e8Pb2xrhx43D+/Hlp+cWLF6FSqbB9+3YMHz4cLi4u6NmzJ7777rv72v8///lPTJ06FTExMejUqRO6d++Op556Cm+99RaA386aN23ahM8//xwqlQoqlQqZmZkAgFdeeQWdO3eGi4sLOnbsiMWLF0t3jaampuKNN97Ajz/+KK2Xmpoq1Xv8+HGphtLSUrPtlpSUYMaMGWjTpg2cnZ0RHBxs9theInrweMb9H1evXsXYsWMRHR2NzZs346effsKsWbPQqlUrJCUlAQAqKioQFxeH0NBQVFRU4PXXX8ekSZNw/PhxszdWLFq0CO+++y6Cg4OxaNEiPPXUUzh37hwcHO7+dWu1WmRlZeHSpUsIDAysszwhIQGnT5+G0WiUwtPLywvAb88oT01Nhb+/P3JzczFr1iy4u7tjwYIFmDZtGvLy8rBnzx7s378fwG83Q/3888/3/F4WL16MU6dO4csvv4SPjw/OnTuHysrK+/pOiaiZWPTqmYeEwWAQAITBYKizrLKyUpw6dUpUVlbWu25UVJR4/PHH68xfuHCh6NKli6itrZXm/fWvfxVubm6ipqam3m0VFRUJACI3N1cIIUR+fr4AINavXy+1OXnypAAgTp8+fc/junbtmhgwYIAAIDp37iyioqLEtm3bzPbfUP13Wr58uQgLC5M+L1myRPTs2dOsze16jx07Js0rKSkRAMTXX38thBBi/PjxYubMmffc3233+v6p5fht0N3DOT2M7pZrd2JXyX+cPn0aAwcONBuH/Ic//AHl5eW4cuUKAOD8+fOYPn06OnbsCA8PDwQFBQEALl++bLatHj16SH++fev3759L3hCdTofvvvsOubm5+NOf/oTq6mpERUVhzJgxqK2tveu6n332GQYPHgytVgs3NzcsXry4Tl2N8eKLLyItLQ29evXCggUL7vrcdCJ6MBjc/yHqebSs+M8o/dvzx48fj+LiYvztb3/D999/j++//x4AUFVVZbaeo6Oj9Ofb694reH8vJCQEc+fOxSeffIKMjAxkZGQgKyurwfaHDx/Gk08+icjISOzcuRPHjh3DokWL6tR1p9vdO7ePE0CdpylGRkbi0qVLiI2NxbVr1zBy5EgkJCTc97EQkfUxuP+jW7duyM7ONgux7OxsuLu7o23btiguLsbp06fx2muvYeTIkejatStKSkoeSF3Ab/3rwG+P1a2pqTFrc+jQIQQGBmLRokXo27cvgoODcenSJbM29a3Xpk0bAL89ofG231+o/H276OhobN26FatWrcK6deuafFxE1Hgt8uKkwWCoE1DPP/88Vq1ahfnz52PevHk4c+YMlixZgri4ONjZ2aF169bw9vbGunXroNPpcPnyZbz66qtWrevFF1+Ev78/RowYgXbt2qGwsBBvv/022rRpI70erkOHDti7dy/OnDkDb29vaDQadOrUCZcvX0ZaWhr69euHXbt2IT093WzbHTp0QH5+Po4fP4527drB3d0dzs7OGDBgAJYuXYoOHTrg+vXreO2118zWe/311xEWFobu3bvDZDJh586d6Nq1q1WPm4gs1Mz97c2iqRcnAdSZoqKiRGZmpujXr59wcnISWq1WvPLKK6K6ulpaNyMjQ3Tt2lWo1WrRo0cPkZmZKQCI9PR0IcT9Xey7m88++0yMHTtW6HQ64eTkJPz9/cWUKVPEiRMnpDZFRUVi1KhRws3NzWy7L7/8svD29hZubm5i2rRpYuXKlUKj0Ujr3bp1S0yZMkV4enoKAGLjxo1CCCFOnTolBgwYIJydnUWvXr3Evn37zLb71ltvia5duwpnZ2fh5eUlHn/8cXHhwoUGj4EXJ+k2W1+AVPLFScU+HZBPp7MNfv90Gx8yZZkH8pZ3IiKyDQb3AxQZGQk3N7d6p+TkZFuXR0Qy0SIvTtrK+vXrG7zr8PYdkERE98LgfoDatm1r6xKISAHYVUJEJDMMbiIimWFwExHJjEXBvXbtWvTo0QMeHh7w8PDAwIED8eWXX0rLhRBISkqCv78/nJ2dMWzYMJw8edJsGyaTCfPnz4ePjw9cXV0xYcIE6SFORER0bxYFd7t27bB06VIcPXoUR48exYgRI/D4449L4bx8+XKsWLECa9aswZEjR6DVajFq1CiUlZVJ24iNjUV6ejrS0tJw8OBBlJeXY9y4cXWeo0FERA1o6m2arVu3FuvXrxe1tbVCq9WKpUuXSstu3bolNBqN+PDDD4UQQpSWlgpHR0eRlpYmtbl69aqws7MTe/bsue99NuWWd2pe/P7pNlvf1q7kW94bPRywpqYGf//731FRUYGBAwciPz8fer0eERERUhu1Wo2hQ4ciOzsbL7zwAnJyclBdXW3Wxt/fHyEhIcjOzsbo0aOb8jvogal+I/6B7ctxyXsWrxMdHY1NmzYhJSXF7EFYO3bswKRJk8yegEhE8mPxxcnc3Fy4ublBrVZj9uzZSE9PR7du3aDX6wEAfn5+Zu39/PykZXq9Hk5OTmjdunWDbepjMplgNBrNJrq7Vq1aYdmyZQ/k0bNE9GBZHNxdunTB8ePHcfjwYbz44ouIiooyewt5fS8juHPene7VJiUlBRqNRpoCAgIsLbvFCQ8Ph1arRUpKiq1LISIrszi4nZyc0KlTJ/Tt2xcpKSno2bMnVq9eDa1WCwB1zpyLioqks3CtVouqqqo6Z4G/b1OfxMREGAwGaSooKLC07BbH3t4eycnJeP/99zlqh0hhmjyOWwgBk8mEoKAgaLVaZGRkSMuqqqqQlZWFQYMGAQDCwsLg6Oho1qawsBB5eXlSm/qo1WppCOLtie5t0qRJ6NWrF5YsWWLrUojIiiy6OLlw4UJERkYiICAAZWVlSEtLQ2ZmJvbs2QOVSoXY2FgkJycjODgYwcHBSE5OhouLC6ZPnw4A0Gg0iImJQXx8PLy9veHl5YWEhASEhoYiPDy8WQ6wpVu2bBlGjBiB+PgHd0GViJqXRcH9888/45lnnkFhYSE0Gg169OiBPXv2YNSoUQCABQsWoLKyEnPmzEFJSQn69++Pffv2wd3dXdrGypUr4eDggKlTp6KyshIjR45Eamoq7O3trXtkBAAYMmQIRo8ejYULFyI6OtrW5RCRFVgU3Bs2bLjrcpVKhaSkJCQlJTXYplWrVnj//ffx/vvvW7JraoKlS5eiV69e6Ny5s61LISIr4LNKWoDQ0FDMmDGDvyyJFILB3UK89dZbvPGGSCH4IoVGaMzdjA9SampqnXmBgYG4devWgy+GiKyOZ9xERDLD4CYikhkGNxGRzDC4iYhkhsFNRCQzDG4iIplhcBMRyQyDm4hIZhjcREQyw+AmIpIZBncjqFQPbrKEEALh4eH1vnT5gw8+gEajweXLl630LRCRrTC4FUSlUmHjxo34/vvv8dFHH0nz8/Pz8corr2D16tVo3769DSskImtgcCtMQEAAVq9ejYSEBOTn50MIgZiYGIwcOZIvUiBSCD4dUIGioqKQnp6OmTNnYsqUKcjLy0NeXp6tyyIiK2FwK9S6desQEhKCb7/9Fp999hl8fX1tXRIRWQm7ShTK19cXzz//PLp27YpJkybZuhwisiIGt4I5ODjAwYH/qSJSGgY3EZHMMLiJiGTGouBOSUlBv3794O7uDl9fX0ycOBFnzpwxaxMdHQ2VSmU2DRgwwKyNyWTC/Pnz4ePjA1dXV0yYMAFXrlxp+tEQEbUAFgV3VlYW5s6di8OHDyMjIwO//vorIiIiUFFRYdZuzJgxKCwslKbdu3ebLY+NjUV6ejrS0tJw8OBBlJeXY9y4caipqWn6ET0AQjy4qSmSkpJw/PhxqxwzET08LLpytWfPHrPPGzduhK+vL3JycjBkyBBpvlqthlarrXcbBoMBGzZswJYtWxAeHg4A2Lp1KwICArB///56b9cmIqL/16Q+boPBAADw8vIym5+ZmQlfX1907twZs2bNQlFRkbQsJycH1dXViIiIkOb5+/sjJCQE2dnZ9e7HZDLBaDSaTURELVWjg1sIgbi4OAwePBghISHS/MjISHzyySc4cOAA3nvvPRw5cgQjRoyAyWQCAOj1ejg5OaF169Zm2/Pz84Ner693XykpKdBoNNIUEBDQ2LKJiGSv0YN8582bhxMnTuDgwYNm86dNmyb9OSQkBH379kVgYCB27dqFyZMnN7g9IQRUDTwOLzExEXFxcdJno9HI8CaiFqtRZ9zz58/HF198ga+//hrt2rW7a1udTofAwECcPXsWAKDValFVVYWSkhKzdkVFRfDz86t3G2q1Gh4eHmYTEVFLZVFwCyEwb948bN++HQcOHEBQUNA91ykuLkZBQQF0Oh0AICwsDI6OjsjIyJDaFBYWIi8vD4MGDbKw/LvXSg8ev3ei5mdRV8ncuXPx6aef4vPPP4e7u7vUJ63RaODs7Izy8nIkJSVhypQp0Ol0uHjxIhYuXAgfHx/peRkajQYxMTGIj4+Ht7c3vLy8kJCQgNDQUGmUSVM4OjoCAG7evAlnZ+cmb48sc/PmTQD///dARNZnUXCvXbsWADBs2DCz+Rs3bkR0dDTs7e2Rm5uLzZs3o7S0FDqdDsOHD8e2bdvg7u4utV+5ciUcHBwwdepUVFZWYuTIkUhNTYW9vX2TD8je3h6enp7SSBYXF5cG+87JeoQQuHnzJoqKiuDp6WmVv0siqp9KyPD/tkajERqNBgaDod7+biEE9Ho9SktLH3xxLZynpye0Wi1/WZLFr957kB7G1LtXrv2eIh8dp1KpoNPp4Ovri+rqaluX02I4OjryTJvoAVBkcN9mb2/PICEixeHTAYmIZIbBTUQkMwxuIiKZYXATEckMg5uISGYY3EREMsPgJiKSGQY3EZHMMLiJiGSGwU1EJDMMbiIimWFwExHJDIObiEhmGNxERDLD4CYikhkGNxGRzDC4iYhkhsFNRCQzDG4iIpmxKLhTUlLQr18/uLu7w9fXFxMnTsSZM2fM2gghkJSUBH9/fzg7O2PYsGE4efKkWRuTyYT58+fDx8cHrq6umDBhAq5cudL0oyEiagEsCu6srCzMnTsXhw8fRkZGBn799VdERESgoqJCarN8+XKsWLECa9aswZEjR6DVajFq1CiUlZVJbWJjY5Geno60tDQcPHgQ5eXlGDduHGpqaqx3ZERESiWaoKioSAAQWVlZQgghamtrhVarFUuXLpXa3Lp1S2g0GvHhhx8KIYQoLS0Vjo6OIi0tTWpz9epVYWdnJ/bs2XNf+zUYDAKAMBgMTSmfiJoR8PBODyNLcq1JfdwGgwEA4OXlBQDIz8+HXq9HRESE1EatVmPo0KHIzs4GAOTk5KC6utqsjb+/P0JCQqQ2RETUMIfGriiEQFxcHAYPHoyQkBAAgF6vBwD4+fmZtfXz88OlS5ekNk5OTmjdunWdNrfXv5PJZILJZJI+G43GxpZNRCR7jT7jnjdvHk6cOIH/+Z//qbNMpVKZfRZC1Jl3p7u1SUlJgUajkaaAgIDGlk1EJHuNCu758+fjiy++wNdff4127dpJ87VaLQDUOXMuKiqSzsK1Wi2qqqpQUlLSYJs7JSYmwmAwSFNBQUFjyiYiUgSLglsIgXnz5mH79u04cOAAgoKCzJYHBQVBq9UiIyNDmldVVYWsrCwMGjQIABAWFgZHR0ezNoWFhcjLy5Pa3EmtVsPDw8NsIiJqqSzq4547dy4+/fRTfP7553B3d5fOrDUaDZydnaFSqRAbG4vk5GQEBwcjODgYycnJcHFxwfTp06W2MTExiI+Ph7e3N7y8vJCQkIDQ0FCEh4db/wiJiJTGkuEqAOqdNm7cKLWpra0VS5YsEVqtVqjVajFkyBCRm5trtp3Kykoxb9484eXlJZydncW4cePE5cuX77sODgckevjZesifkocDqoQQwna/NhrHaDRCo9HAYDCw24ToIXWP8Qg29TCmniW5xmeVEBHJDIObiEhmGNxERDLD4CYikhkGNxGRzDC4iYhkhsFNRCQzDG4iIplhcBMRyQyDm4hIZhjcREQyw+AmIpIZBjcRkcwwuImIZIbBTUQkMwxuIiKZYXATEckMg5uISGYY3EREMsPgJiKSGQY3EZHMMLiJiGTG4uD+5ptvMH78ePj7+0OlUmHHjh1my6Ojo6FSqcymAQMGmLUxmUyYP38+fHx84OrqigkTJuDKlStNOhAiopbC4uCuqKhAz549sWbNmgbbjBkzBoWFhdK0e/dus+WxsbFIT09HWloaDh48iPLycowbNw41NTWWHwERUQvjYOkKkZGRiIyMvGsbtVoNrVZb7zKDwYANGzZgy5YtCA8PBwBs3boVAQEB2L9/P0aPHm1pSURELUqz9HFnZmbC19cXnTt3xqxZs1BUVCQty8nJQXV1NSIiIqR5/v7+CAkJQXZ2dr3bM5lMMBqNZhMRUUtl8Rn3vURGRuKJJ55AYGAg8vPzsXjxYowYMQI5OTlQq9XQ6/VwcnJC69atzdbz8/ODXq+vd5spKSl44403rF0qkexVvxFv6xLu4j1bF6BYVg/uadOmSX8OCQlB3759ERgYiF27dmHy5MkNrieEgEqlqndZYmIi4uLipM9GoxEBAQHWK5qISEaafTigTqdDYGAgzp49CwDQarWoqqpCSUmJWbuioiL4+fnVuw21Wg0PDw+ziYiopWr24C4uLkZBQQF0Oh0AICwsDI6OjsjIyJDaFBYWIi8vD4MGDWrucoiIZM/irpLy8nKcO3dO+pyfn4/jx4/Dy8sLXl5eSEpKwpQpU6DT6XDx4kUsXLgQPj4+mDRpEgBAo9EgJiYG8fHx8Pb2hpeXFxISEhAaGiqNMiEiooZZHNxHjx7F8OHDpc+3+56joqKwdu1a5ObmYvPmzSgtLYVOp8Pw4cOxbds2uLu7S+usXLkSDg4OmDp1KiorKzFy5EikpqbC3t7eCodERKRsKiGEsHURljIajdBoNDAYDOzvphbtYR5V4pT08I4qeRhTz5Jc47NKiIhkhsFNRCQzDG4iIplhcBMRyQyDm4hIZhjcREQyw+AmIpIZBjcRkcwwuImIZIbBTUQkMwxuIiKZYXATEckMg5uISGYY3EREMsPgJiKSGQY3EZHMMLiJiGSGwU1EJDMMbiIimWFwExHJDIObiEhmLA7ub775BuPHj4e/vz9UKhV27NhhtlwIgaSkJPj7+8PZ2RnDhg3DyZMnzdqYTCbMnz8fPj4+cHV1xYQJE3DlypUmHQgRUUthcXBXVFSgZ8+eWLNmTb3Lly9fjhUrVmDNmjU4cuQItFotRo0ahbKyMqlNbGws0tPTkZaWhoMHD6K8vBzjxo1DTU1N44+EiKiFcLB0hcjISERGRta7TAiBVatWYdGiRZg8eTIAYNOmTfDz88Onn36KF154AQaDARs2bMCWLVsQHh4OANi6dSsCAgKwf/9+jB49ugmHQ0SkfFbt487Pz4der0dERIQ0T61WY+jQocjOzgYA5OTkoLq62qyNv78/QkJCpDZERNQwi8+470av1wMA/Pz8zOb7+fnh0qVLUhsnJye0bt26Tpvb69/JZDLBZDJJn41GozXLJiKSlWYZVaJSqcw+CyHqzLvT3dqkpKRAo9FIU0BAgNVqJSKSG6sGt1arBYA6Z85FRUXSWbhWq0VVVRVKSkoabHOnxMREGAwGaSooKLBm2UREsmLV4A4KCoJWq0VGRoY0r6qqCllZWRg0aBAAICwsDI6OjmZtCgsLkZeXJ7W5k1qthoeHh9lERNRSWdzHXV5ejnPnzkmf8/Pzcfz4cXh5eaF9+/aIjY1FcnIygoODERwcjOTkZLi4uGD69OkAAI1Gg5iYGMTHx8Pb2xteXl5ISEhAaGioNMqEiIgaZnFwHz16FMOHD5c+x8XFAQCioqKQmpqKBQsWoLKyEnPmzEFJSQn69++Pffv2wd3dXVpn5cqVcHBwwNSpU1FZWYmRI0ciNTUV9vb2VjgkIiJlUwkhhK2LsJTRaIRGo4HBYGC3CbVo1W/E27qEBjklvWfrEhr0MKaeJbnGZ5UQEckMg5uISGYY3EREMsPgJiKSGQY3EZHMMLiJiGSGwU1EJDMMbiIimWFwExHJDIObiEhmrPoiBXr43eOx6Db1MN6GTPQw4hk3EZHMMLiJiGSGwU1EJDMMbiIimWFwExHJDIObiEhmGNxERDLD4CYikhkGNxGRzDC4iYhkhsFNRCQzVg/upKQkqFQqs0mr1UrLhRBISkqCv78/nJ2dMWzYMJw8edLaZRARKVaznHF3794dhYWF0pSbmystW758OVasWIE1a9bgyJEj0Gq1GDVqFMrKypqjFCIixWmW4HZwcIBWq5WmNm3aAPjtbHvVqlVYtGgRJk+ejJCQEGzatAk3b97Ep59+2hylEBEpTrME99mzZ+Hv74+goCA8+eSTuHDhAgAgPz8fer0eERERUlu1Wo2hQ4ciOzu7we2ZTCYYjUaziYiopbJ6cPfv3x+bN2/G3r178be//Q16vR6DBg1CcXEx9Ho9AMDPz89sHT8/P2lZfVJSUqDRaKQpICDA2mUTEcmG1YM7MjISU6ZMQWhoKMLDw7Fr1y4AwKZNm6Q2qjue5i+EqDPv9xITE2EwGKSpoKDA2mUTEclGsw8HdHV1RWhoKM6ePSuNLrnz7LqoqKjOWfjvqdVqeHh4mE1ERC1Vswe3yWTC6dOnodPpEBQUBK1Wi4yMDGl5VVUVsrKyMGjQoOYuhYhIEaz+zsmEhASMHz8e7du3R1FREd5++20YjUZERUVBpVIhNjYWycnJCA4ORnBwMJKTk+Hi4oLp06dbuxQiIkWyenBfuXIFTz31FK5fv442bdpgwIABOHz4MAIDAwEACxYsQGVlJebMmYOSkhL0798f+/btg7u7u7VLISJSJJUQ8nu3ttFohEajgcFgYH+3hfiWd2WpfiPe1iU0yCnpPVuX0KCH8WfNklzjs0qIiGTG6l0l9HCfBQEP71kQEd0fnnETEckMg5uISGYY3EREMsPgJiKSGQY3EZHMMLiJiGSGwU1EJDMMbiIimWFwExHJDIObiEhmGNxERDLD4CYikhkGNxGRzDC4iYhkhsFNRCQzDG4iIplhcBMRyQyDm4hIZhjcREQyY9Pg/uCDDxAUFIRWrVohLCwM3377rS3LISKSBZsF97Zt2xAbG4tFixbh2LFjePTRRxEZGYnLly/bqiQiIlmwWXCvWLECMTEx+OMf/4iuXbti1apVCAgIwNq1a21VEhGRLDjYYqdVVVXIycnBq6++ajY/IiIC2dnZddqbTCaYTCbps8FgAAAYjcbmLbSRqm+Z7t3IZh7O7wwAHtK/zocaf9Ya52H8WbudZ0KIe7a1SXBfv34dNTU18PPzM5vv5+cHvV5fp31KSgreeOONOvMDAgKarUbl+qutC2iQRmPrCsi6+LPWGGVlZdDco0CbBPdtKpXK7LMQos48AEhMTERcXJz0uba2Fjdu3IC3t3e97al+RqMRAQEBKCgogIeHh63LIQXjz5rlhBAoKyuDv7//PdvaJLh9fHxgb29f5+y6qKiozlk4AKjVaqjVarN5np6ezVmionl4ePAfEz0Q/FmzzL3OtG+zycVJJycnhIWFISMjw2x+RkYGBg0aZIuSiIhkw2ZdJXFxcXjmmWfQt29fDBw4EOvWrcPly5cxe/ZsW5VERCQLNgvuadOmobi4GG+++SYKCwsREhKC3bt3IzAw0FYlKZ5arcaSJUvqdDsRWRt/1pqXStzP2BMiInpo8FklREQyw+AmIpIZBjcRkcwwuImIZIbBTUQkMwxuIiKZYXATkVWdO3cOe/fuRWVlJYD7e9odWYbB3QKUlpZi/fr1SExMxI0bNwAAP/zwA65evWrjykhJiouLER4ejs6dO2Ps2LEoLCwEAPzxj39EfHy8jatTFga3wp04cQKdO3fGsmXL8O6776K0tBQAkJ6ejsTERNsWR4ry5z//GQ4ODrh8+TJcXFyk+dOmTcOePXtsWJnyMLgVLi4uDtHR0Th79ixatWolzY+MjMQ333xjw8pIafbt24dly5ahXbt2ZvODg4Nx6dIlG1WlTAxuhTty5AheeOGFOvPbtm1b70sriBqroqLC7Ez7tuvXr/OZJVbG4Fa4Vq1a1fuKtzNnzqBNmzY2qIiUasiQIdi8ebP0WaVSoba2Fu+88w6GDx9uw8qUhw+ZUrjnn38ev/zyC/73f/8XXl5eOHHiBOzt7TFx4kQMGTIEq1atsnWJpBCnTp3CsGHDEBYWhgMHDmDChAk4efIkbty4gUOHDuGRRx6xdYmKweBWOKPRiLFjx+LkyZPSa5H0ej0GDhyI3bt3w9XV1dYlkoLo9XqsXbsWOTk5qK2tRZ8+fTB37lzodDpbl6YoDO4W4sCBA/jhhx+kf0zh4eG2LomIGonBTUSNduLEiftu26NHj2aspGVhcCvQX/7yl/tu+6c//akZKyGls7Ozg0qluufdkSqVCjU1NQ+oKuVjcCtQUFDQfbVTqVS4cOFCM1dDSmbJ+Gy+ltB6GNxERDJjs5cFE5EynTp1CpcvX0ZVVZXZ/AkTJtioIuVhcLcAV65cwRdffFHvP6YVK1bYqCpSmgsXLmDSpEnIzc016/dWqVQAwD5uK2JwK9xXX32FCRMmICgoCGfOnEFISAguXrwIIQT69Olj6/JIQV566SUEBQVh//796NixI/71r3+huLgY8fHxePfdd21dnqLwlneFS0xMRHx8PPLy8tCqVSv84x//QEFBAYYOHYonnnjC1uWRgnz33Xd488030aZNG9jZ2cHOzg6DBw9GSkoKRy9ZGYNb4U6fPo2oqCgAgIODAyorK+Hm5oY333wTy5Yts3F1pCQ1NTVwc3MDAPj4+ODatWsAfhtNcubMGVuWpjgMboVzdXWFyWQCAPj7++P8+fPSsuvXr9uqLFKgkJAQ6Yac/v37Y/ny5Th06BDefPNNdOzY0cbVKQv7uBVuwIABOHToELp164bHHnsM8fHxyM3Nxfbt2zFgwABbl0cK8tprr6GiogIA8Pbbb2PcuHF49NFH4e3tjbS0NBtXpywcx61wFy5cQHl5OXr06IGbN28iISEBBw8eRKdOnbBy5UreFEHN6saNG2jdurU0soSsg8FNRE3y3HPP3Ve7jz/+uJkraTkY3C1IeXk5amtrzeZ5eHjYqBpSCjs7OwQGBqJ37953fWZJenr6A6xK2RjcCpefn4958+YhMzMTt27dkuYLIfjgH7KKOXPmIC0tDe3bt8dzzz2Hp59+Gl5eXrYuS9EY3Ao3aNAgAL/dHOHn51enr3Ho0KG2KIsUxmQyYfv27fj444+RnZ2Nxx57DDExMYiIiGD/djNgcCucm5sbcnJy0KVLF1uXQi3EpUuXkJqais2bN6O6uhqnTp2SxneTdXAct8L169cPBQUFti6DWhCVSiU9q+TOaypkHTzjVrjz589j9uzZePrppxESEgJHR0ez5XwrCVnD77tKDh48iHHjxmHmzJkYM2YM7Ox4fmhtvAFH4X755RecP38eM2fOlObdPhvixUmyht9fnJw5cybS0tLg7e1t67IUjWfcCtetWzd07doVCxYsqPfiJG/Aoaays7ND+/bt0bt377teiNy+ffsDrErZeMatcJcuXcIXX3yBTp062boUUqhnn32WI0ceMAa3wo0YMQI//vgjg5uaTWpqqq1LaHEY3Ao3fvx4/PnPf0Zubi5CQ0PrXJzk66SI5Id93Ap3tyv6vDhJJE8MbiIimeEAyxbk988qISL5YnArXE1NDd566y20bdsWbm5uuHDhAgBg8eLF2LBhg42rI6LGYHAr3H//938jNTUVy5cvh5OTkzQ/NDQU69evt2FlRNRYDG6F27x5M9atW4cZM2bA3t5emt+jRw/89NNPNqyMiBqLwa1wV69erXcMd21tLaqrq21QERE1FYNb4bp3745vv/22zvy///3v6N27tw0qIqKm4g04CrdkyRI888wzuHr1Kmpra7F9+3acOXMGmzdvxs6dO21dHhE1AsdxK9SFCxcQFBQElUqFvXv3Ijk5GTk5OaitrUWfPn3w+uuvIyIiwtZlElEjMLgVyt7eHoWFhfD19QUATJs2DatXr4ZWq7VxZUTUVOzjVqg7fx9/+eWXuHnzpo2qISJrYnC3EPyPFZFyMLgV6vZ7/+6cR0Tyx1ElCiWEQHR0NNRqNYDfnlMye/ZsuLq6mrXjW0mI5IfBrVBRUVFmn59++mkbVUJE1sZRJUREMsM+biIimWFwExHJDIObiEhmGNxEVjBs2DDExsbaugxqIRjcpBh6vR4vvfQSOnXqhFatWsHPzw+DBw/Ghx9+yLtGSVE4HJAU4cKFC/jDH/4AT09PJCcnIzQ0FL/++iv+/e9/4+OPP4a/vz8mTJhg6zIbVFNTA5VKBTs7nkvRvfGnhBRhzpw5cHBwwNGjRzF16lR07doVoaGhmDJlCnbt2oXx48cDAAwGA55//nn4+vrCw8MDI0aMwI8//ihtJykpCb169cKWLVvQoUMHaDQaPPnkkygrK5PaVFRU4Nlnn4Wbmxt0Oh3ee++9OvVUVVVhwYIFaNu2LVxdXdG/f39kZmZKy1NTU+Hp6YmdO3eiW7duUKvVuHTpUvN9QaQoDG6SveLiYuzbtw9z586tc2fobSqVCkIIPPbYY9Dr9di9ezdycnLQp08fjBw5Ejdu3JDanj9/Hjt27MDOnTuxc+dOZGVlYenSpdLyl19+GV9//TXS09Oxb98+ZGZmIicnx2x/M2fOxKFDh5CWloYTJ07giSeewJgxY3D27Fmpzc2bN5GSkoL169fj5MmT0pMcie5JEMnc4cOHBQCxfft2s/ne3t7C1dVVuLq6igULFoivvvpKeHh4iFu3bpm1e+SRR8RHH30khBBiyZIlwsXFRRiNRmn5yy+/LPr37y+EEKKsrEw4OTmJtLQ0aXlxcbFwdnYWL730khBCiHPnzgmVSiWuXr1qtp+RI0eKxMREIYQQGzduFADE8ePHrfMlUIvCPm5SjDsfovWvf/0LtbW1mDFjBkwmE3JyclBeXg5vb2+zdpWVlTh//rz0uUOHDnB3d5c+63Q6FBUVAfjtbLyqqgoDBw6Ulnt5eaFLly7S5x9++AFCCHTu3NlsPyaTyWzfTk5O6NGjRxOOmFoqBjfJXqdOnaBSqeq8tb5jx44AAGdnZwC/vSBZp9OZ9TXf5unpKf3Z0dHRbJlKpUJtbS2A+3s8bm1tLezt7ZGTkwN7e3uzZW5ubtKfnZ2d+cRGahQGN8met7c3Ro0ahTVr1mD+/PkN9nP36dMHer0eDg4O6NChQ6P21alTJzg6OuLw4cNo3749AKCkpAT//ve/MXToUABA7969UVNTg6KiIjz66KON2g/R3fDiJCnCBx98gF9//RV9+/bFtm3bcPr0aZw5cwZbt27FTz/9BHt7e4SHh2PgwIGYOHEi9u7di4sXLyI7OxuvvfYajh49el/7cXNzQ0xMDF5++WV89dVXyMvLQ3R0tNkwvs6dO2PGjBl49tlnsX37duTn5+PIkSNYtmwZdu/e3VxfAbUgPOMmRXjkkUdw7NgxJCcnIzExEVeuXIFarUa3bt2QkJCAOXPmQKVSYffu3Vi0aBGee+45/PLLL9BqtRgyZAj8/Pzue1/vvPMOysvLMWHCBLi7uyM+Ph4Gg8GszcaNG/H2228jPj4eV69ehbe3NwYOHIixY8da+9CpBeJjXYmIZIZdJUREMsPgJiKSGQY3EZHMMLiJiGSGwU1EJDMMbiIimWFwExHJDIObiEhmGNxERDLD4CYikhkGNxGRzDC4iYhk5v8APW4X6HwImQQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y.plot(kind=\"bar\",color=[\"salmon\",\"blue\"],figsize=(4,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb395e6",
   "metadata": {},
   "source": [
    "Damn, i guess we will have to first handle missing values\n",
    "\n",
    "les go then,\n",
    "### now we have two options \n",
    "first to remove all the rows with missing values but this can result in data loss and not working of the model correctly\n",
    "second is to replace the missing values, lets try the first method \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7035fc39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loan_ID               0\n",
       "Gender               13\n",
       "Married               3\n",
       "Dependents           15\n",
       "Education             0\n",
       "Self_Employed        32\n",
       "ApplicantIncome       0\n",
       "CoapplicantIncome     0\n",
       "LoanAmount           22\n",
       "Loan_Amount_Term     14\n",
       "Credit_History       50\n",
       "Property_Area         0\n",
       "Loan_Status           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_set_df.dropna(subset=[\"Gender\",\"Married\",\"Dependents\",\"Self_Employed\",\"LoanAmount\",\"Loan_Amount_Term\",\n",
    "#                            \"Credit_History\"], inplace=True)\n",
    "train_set_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd112499",
   "metadata": {},
   "source": [
    "lmao there are no missing values in the target column,mb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "94029798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "614"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as we can see the data loss is too much, hence we will use the second option that is to \n",
    "# be rplacing values  now using imputer\n",
    "len(train_set_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b3823e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## replacing missing values using imputer\n",
    "from sklearn.impute import SimpleImputer as SI\n",
    "from sklearn.compose import ColumnTransformer as CT\n",
    "# now here we will be filling string values with missing and integer values with mean\n",
    "typ1 = SI(strategy=\"constant\", fill_value=\"missing\")\n",
    "# Married_feature = SI(strategy=\"constant\", fill_value=\"missing\")\n",
    "typ2 = SI(strategy=\"constant\", fill_value=\"1\")\n",
    "# Self_emp_feature = SI(strategy=\"constant\", fill_value=\"missing\")\n",
    "typ3 = SI(strategy=\"mean\")\n",
    "col1 = [\"Loan_ID\",\"Gender\",\"Married\",\"Self_Employed\",\"ApplicantIncome\",\"CoapplicantIncome\",\"Property_Area\"]\n",
    "col2 = [\"Dependents\",\"Education\",\"Credit_History\"]\n",
    "col3 = [\"LoanAmount\",\"Loan_Amount_Term\"]\n",
    "# creating an imputer , SOmething that will fill on the missing data\n",
    "imputer = CT([(\"typ1\",typ1,col1),\n",
    "              (\"typ2\",typ2,col2),\n",
    "              (\"typ3\",typ3,col3)])\n",
    "\n",
    "# filled training set\n",
    "filled_train_Set = imputer.fit_transform(xtrain_set_df_features)\n",
    "len(filled_train_Set)\n",
    "\n",
    "##filling the new data into a new dataframe \n",
    "missing_val_filled_train_set_df = pd.DataFrame(filled_train_Set,\n",
    "                                              columns=[\"Loan_ID\",\"Gender\",\"Married\",\"Self_Employed\",\"ApplicantIncome\",\n",
    "                                                       \"CoapplicantIncome\",\"Property_Area\",\n",
    "                                                       \"Dependents\",\"Education\",\"Credit_History\",\"LoanAmount\",\n",
    "                                                       \"Loan_Amount_Term\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e93a53f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loan_ID              0\n",
       "Gender               0\n",
       "Married              0\n",
       "Self_Employed        0\n",
       "ApplicantIncome      0\n",
       "CoapplicantIncome    0\n",
       "Property_Area        0\n",
       "Dependents           0\n",
       "Education            0\n",
       "Credit_History       0\n",
       "LoanAmount           0\n",
       "Loan_Amount_Term     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## NOW LETS CHECK WETHER WE FILLED THE MISSING VALUES OR NOT\n",
    "missing_val_filled_train_set_df.isna().sum()\n",
    "# train_set_df\n",
    "# filled_train_Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8700d62d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "614"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YES WE FILLED IT,  LETS CHECK THE LENGTH OF THE DATASET TO CHECK FOR DATA LOSS\n",
    "len(missing_val_filled_train_set_df)\n",
    "# NO DATA LOSS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d0d43d",
   "metadata": {},
   "source": [
    "## Doing the data conversion from other dtypes to int\n",
    "so that it would be easier for our machine to work on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d9cd9b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loan_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Married</th>\n",
       "      <th>Self_Employed</th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>Property_Area</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Education</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LP001002</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>5849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>1.0</td>\n",
       "      <td>146.412162</td>\n",
       "      <td>360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LP001003</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>4583</td>\n",
       "      <td>1508.0</td>\n",
       "      <td>Rural</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>1.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LP001005</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>1.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LP001006</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>2583</td>\n",
       "      <td>2358.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>0</td>\n",
       "      <td>Not Graduate</td>\n",
       "      <td>1.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LP001008</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>1.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>LP002978</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Rural</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>1.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>LP002979</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>4106</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Rural</td>\n",
       "      <td>3+</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>LP002983</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>8072</td>\n",
       "      <td>240.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>1.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>LP002984</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>7583</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>2</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>1.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>LP002990</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>4583</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Semiurban</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>0.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>360.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>614 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Loan_ID  Gender Married Self_Employed ApplicantIncome CoapplicantIncome  \\\n",
       "0    LP001002    Male      No            No            5849               0.0   \n",
       "1    LP001003    Male     Yes            No            4583            1508.0   \n",
       "2    LP001005    Male     Yes           Yes            3000               0.0   \n",
       "3    LP001006    Male     Yes            No            2583            2358.0   \n",
       "4    LP001008    Male      No            No            6000               0.0   \n",
       "..        ...     ...     ...           ...             ...               ...   \n",
       "609  LP002978  Female      No            No            2900               0.0   \n",
       "610  LP002979    Male     Yes            No            4106               0.0   \n",
       "611  LP002983    Male     Yes            No            8072             240.0   \n",
       "612  LP002984    Male     Yes            No            7583               0.0   \n",
       "613  LP002990  Female      No           Yes            4583               0.0   \n",
       "\n",
       "    Property_Area Dependents     Education Credit_History  LoanAmount  \\\n",
       "0           Urban          0      Graduate            1.0  146.412162   \n",
       "1           Rural          1      Graduate            1.0       128.0   \n",
       "2           Urban          0      Graduate            1.0        66.0   \n",
       "3           Urban          0  Not Graduate            1.0       120.0   \n",
       "4           Urban          0      Graduate            1.0       141.0   \n",
       "..            ...        ...           ...            ...         ...   \n",
       "609         Rural          0      Graduate            1.0        71.0   \n",
       "610         Rural         3+      Graduate            1.0        40.0   \n",
       "611         Urban          1      Graduate            1.0       253.0   \n",
       "612         Urban          2      Graduate            1.0       187.0   \n",
       "613     Semiurban          0      Graduate            0.0       133.0   \n",
       "\n",
       "    Loan_Amount_Term  \n",
       "0              360.0  \n",
       "1              360.0  \n",
       "2              360.0  \n",
       "3              360.0  \n",
       "4              360.0  \n",
       "..               ...  \n",
       "609            360.0  \n",
       "610            180.0  \n",
       "611            360.0  \n",
       "612            360.0  \n",
       "613            360.0  \n",
       "\n",
       "[614 rows x 12 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_val_filled_train_set_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "58200569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0, 0)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 617)\\t1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0, 1)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0, 2)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(0, 3)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0, 4)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 617)\\t1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>(0, 609)\\t1.0\\n  (0, 614)\\t1.0\\n  (0, 617)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>(0, 610)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>(0, 611)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>(0, 612)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>(0, 613)\\t1.0\\n  (0, 614)\\t1.0\\n  (0, 617)\\t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>614 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0\n",
       "0      (0, 0)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 617)\\t1....\n",
       "1      (0, 1)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t1....\n",
       "2      (0, 2)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t1....\n",
       "3      (0, 3)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t1....\n",
       "4      (0, 4)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 617)\\t1....\n",
       "..                                                 ...\n",
       "609    (0, 609)\\t1.0\\n  (0, 614)\\t1.0\\n  (0, 617)\\t...\n",
       "610    (0, 610)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t...\n",
       "611    (0, 611)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t...\n",
       "612    (0, 612)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t...\n",
       "613    (0, 613)\\t1.0\\n  (0, 614)\\t1.0\\n  (0, 617)\\t...\n",
       "\n",
       "[614 rows x 1 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## .astype wont work as data has missing values such as NaN\n",
    "#train_set_df.astype(\"int\")\n",
    "## hence we will be usng one hot encoder, to encode all those values\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "#  0   Loan_ID            614 non-null    object \n",
    "#  1   Gender             601 non-null    object \n",
    "#  2   Married            611 non-null    object \n",
    "#  3   Dependents         599 non-null    object \n",
    "#  4   Education          614 non-null    object \n",
    "#  5   Self_Employed      582 non-null    object \n",
    "#  6   ApplicantIncome    614 non-null    int64  \n",
    "#  7   CoapplicantIncome  614 non-null    float64\n",
    "#  8   LoanAmount         592 non-null    float64\n",
    "#  9   Loan_Amount_Term   600 non-null    float64\n",
    "#  10  Credit_History     564 non-null    float64\n",
    "#  11  Property_Area      614 non-null    object \n",
    "#  12  Loan_Status        614 non-null    object\n",
    "missing_val_filled_train_set_df['CoapplicantIncome'] = missing_val_filled_train_set_df['CoapplicantIncome'].astype(str)\n",
    "missing_val_filled_train_set_df['LoanAmount'] = missing_val_filled_train_set_df['LoanAmount'].astype(str)\n",
    "missing_val_filled_train_set_df['Loan_Amount_Term'] = missing_val_filled_train_set_df['Loan_Amount_Term'].astype(str)\n",
    "missing_val_filled_train_set_df['Credit_History'] = missing_val_filled_train_set_df['Credit_History'].astype(str)\n",
    "\n",
    "\n",
    "feature_not_numbers = [\"Loan_ID\",\n",
    "                        \"Gender\",\n",
    "                        \"Married\",\n",
    "                        \"Dependents\",\n",
    "                        \"Education\",\n",
    "                        \"Self_Employed\",\n",
    "                        \"CoapplicantIncome\",\n",
    "                        \"LoanAmount\",\n",
    "                        \"Loan_Amount_Term\",\n",
    "                        \"Credit_History\",\n",
    "                        \"Property_Area\",\n",
    "                        ]\n",
    "one_hot = OneHotEncoder()\n",
    "transformer = CT([(\"one_hot\",one_hot,feature_not_numbers)],\n",
    "                remainder=\"passthrough\")\n",
    "transforemd_data = transformer.fit_transform(missing_val_filled_train_set_df)\n",
    "transforemd_train_Set_df = pd.DataFrame(transforemd_data)\n",
    "\n",
    "#### these are our final transformed features from the training set\n",
    "transforemd_train_Set_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6579499b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loan_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>614 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Loan_Status\n",
       "0             Y\n",
       "1             N\n",
       "2             Y\n",
       "3             Y\n",
       "4             Y\n",
       "..          ...\n",
       "609           Y\n",
       "610           Y\n",
       "611           Y\n",
       "612           Y\n",
       "613           N\n",
       "\n",
       "[614 rows x 1 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_df = pd.DataFrame(ytrain_set_df_target)\n",
    "y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4f597345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>614 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1\n",
       "0    0.0  1.0\n",
       "1    1.0  0.0\n",
       "2    0.0  1.0\n",
       "3    0.0  1.0\n",
       "4    0.0  1.0\n",
       "..   ...  ...\n",
       "609  0.0  1.0\n",
       "610  0.0  1.0\n",
       "611  0.0  1.0\n",
       "612  0.0  1.0\n",
       "613  1.0  0.0\n",
       "\n",
       "[614 rows x 2 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_not_number = [\"Loan_Status\"]\n",
    "# transformer2 = CT([(\"one_hot\",one_hot,target_not_number)],\n",
    "#                  remainder=\"passthrough\")\n",
    "# ytransformed_target = transformer2.fit_transform(y_df)\n",
    "# \n",
    "\n",
    "#### This is our final transformed target from our training set\n",
    "# ytransformed_target_train_set_df\n",
    "transformer = CT([(\"one_hot\",one_hot,target_not_number)],\n",
    "                remainder=\"passthrough\")\n",
    "ytransformed_target = transformer.fit_transform(y_df)\n",
    "ytransformed_target_train_set_df = pd.DataFrame(ytransformed_target)\n",
    "ytransformed_target_train_set_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbc9776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(missing_val_filled_train_set_df.ApplicantIncome[missing_val_filled_train_set_df.Loan_Status==\"Y\"],\n",
    "#             missing_val_filled_train_set_df.Dependents[missing_val_filled_train_set_df.Loan_Status==\"Y\"],\n",
    "#       #      train_set_df.Education[train_set_df.Loan_Status==\"Y\"],\n",
    "#            c=\"salmon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483e1ada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f9b81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(missing_val_filled_train_set_df.ApplicantIncome[missing_val_filled_train_set_df.Loan_Status==\"N\"],\n",
    "#             missing_val_filled_train_set_df.Dependents[missing_val_filled_train_set_df.Loan_Status==\"N\"],\n",
    "#       #      train_set_df.Education[train_set_df.Loan_Status==\"Y\"],\n",
    "#            c=\"salmon\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cdb588",
   "metadata": {},
   "source": [
    "# Checking progress\n",
    "\n",
    " 1) Data was successfully loaded\n",
    "2) Handled the missing values\n",
    "3) data converted to numerical format for the machine to understand easily\n",
    "4) EDA was done on various attributes\n",
    "\n",
    "# Now we will start evaluating our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "97a8d760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 367 entries, 0 to 366\n",
      "Data columns (total 12 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Loan_ID            367 non-null    object \n",
      " 1   Gender             356 non-null    object \n",
      " 2   Married            367 non-null    object \n",
      " 3   Dependents         357 non-null    object \n",
      " 4   Education          367 non-null    object \n",
      " 5   Self_Employed      344 non-null    object \n",
      " 6   ApplicantIncome    367 non-null    int64  \n",
      " 7   CoapplicantIncome  367 non-null    int64  \n",
      " 8   LoanAmount         362 non-null    float64\n",
      " 9   Loan_Amount_Term   361 non-null    float64\n",
      " 10  Credit_History     338 non-null    float64\n",
      " 11  Property_Area      367 non-null    object \n",
      "dtypes: float64(3), int64(2), object(7)\n",
      "memory usage: 34.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# lmao ded , we need to make the test dataset also work ready, damn\n",
    "xtest_Set_df.isna().sum()\n",
    "xtest_Set_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7a075a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "## replacing missing values using imputer\n",
    "from sklearn.impute import SimpleImputer as SI\n",
    "from sklearn.compose import ColumnTransformer as CT\n",
    "# now here we will be filling string values with missing and integer values with mean\n",
    "typ1 = SI(strategy=\"constant\", fill_value=\"missing\")\n",
    "# Married_feature = SI(strategy=\"constant\", fill_value=\"missing\")\n",
    "typ2 = SI(strategy=\"constant\", fill_value=\"1\")\n",
    "# Self_emp_feature = SI(strategy=\"constant\", fill_value=\"missing\")\n",
    "typ3 = SI(strategy=\"mean\")\n",
    "col1 = [\"Loan_ID\",\"Gender\",\"Married\",\"Self_Employed\",\"ApplicantIncome\",\"CoapplicantIncome\",\"Property_Area\"]\n",
    "col2 = [\"Dependents\",\"Education\",\"Credit_History\"]\n",
    "col3 = [\"LoanAmount\",\"Loan_Amount_Term\"]\n",
    "# creating an imputer , SOmething that will fill on the missing d\"ata\n",
    "imputer = CT([(\"typ1\",typ1,col1),\n",
    "              (\"typ2\",typ2,col2),\n",
    "              (\"typ3\",typ3,col3)])\n",
    "\n",
    "# filled training set\n",
    "filled_test_Set = imputer.fit_transform(xtest_Set_df)\n",
    "len(filled_test_Set)\n",
    "\n",
    "##filling the new data into a new dataframe \n",
    "missing_val_filled_test_set_df = pd.DataFrame(filled_test_Set,\n",
    "                                              columns=[\"Loan_ID\",\"Gender\",\"Married\",\"Self_Employed\",\"ApplicantIncome\",\n",
    "                                                       \"CoapplicantIncome\",\"Property_Area\",\n",
    "                                                       \"Dependents\",\"Education\",\"Credit_History\",\"LoanAmount\",\n",
    "                                                       \"Loan_Amount_Term\"])\n",
    "missing_val_filled_test_set_df\n",
    "\n",
    "missing_val_filled_test_set_df['CoapplicantIncome'] = missing_val_filled_test_set_df['CoapplicantIncome'].astype(str)\n",
    "missing_val_filled_test_set_df['LoanAmount'] = missing_val_filled_test_set_df['LoanAmount'].astype(str)\n",
    "missing_val_filled_test_set_df['Loan_Amount_Term'] = missing_val_filled_test_set_df['Loan_Amount_Term'].astype(str)\n",
    "missing_val_filled_test_set_df['Credit_History'] = missing_val_filled_test_set_df['Credit_History'].astype(str)\n",
    "########## converting to numerical format using one hot encoding\n",
    "feature_not_numbers = [\"Loan_ID\",\n",
    "                        \"Gender\",\n",
    "                        \"Married\",\n",
    "                        \"Dependents\",\n",
    "                        \"Education\",\n",
    "                        \"Self_Employed\",\n",
    "                        \"CoapplicantIncome\",\n",
    "                        \"LoanAmount\",\n",
    "                        \"Loan_Amount_Term\",\n",
    "                        \"Credit_History\",\n",
    "                        \"Property_Area\",\n",
    "                        ]\n",
    "one_hot = OneHotEncoder()\n",
    "transformer = CT([(\"one_hot\",one_hot,feature_not_numbers)],\n",
    "                remainder=\"passthrough\")\n",
    "transforemd_test_data = transformer.fit_transform(missing_val_filled_test_set_df)\n",
    "transforemd_test_Set_df = pd.DataFrame(transforemd_data)\n",
    "\n",
    "#### these are our final transformed features from the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "485d8b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0, 0)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 617)\\t1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0, 1)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0, 2)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(0, 3)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0, 4)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 617)\\t1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>(0, 609)\\t1.0\\n  (0, 614)\\t1.0\\n  (0, 617)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>(0, 610)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>(0, 611)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>(0, 612)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>(0, 613)\\t1.0\\n  (0, 614)\\t1.0\\n  (0, 617)\\t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>614 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0\n",
       "0      (0, 0)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 617)\\t1....\n",
       "1      (0, 1)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t1....\n",
       "2      (0, 2)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t1....\n",
       "3      (0, 3)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t1....\n",
       "4      (0, 4)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 617)\\t1....\n",
       "..                                                 ...\n",
       "609    (0, 609)\\t1.0\\n  (0, 614)\\t1.0\\n  (0, 617)\\t...\n",
       "610    (0, 610)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t...\n",
       "611    (0, 611)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t...\n",
       "612    (0, 612)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t...\n",
       "613    (0, 613)\\t1.0\\n  (0, 614)\\t1.0\\n  (0, 617)\\t...\n",
       "\n",
       "[614 rows x 1 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = filled_train_Set\n",
    "y_train = ytransformed_target_train_set_df\n",
    "x_test = filled_test_Set\n",
    "# len(transforemd_train_Set_df)\n",
    "\n",
    "# x_train_array = x_train.toarray().astype(np.float32)\n",
    "# y_train_array = y_train.toarray().astype(np.float32)\n",
    "# we also need to find the values of y_test to complete this code\n",
    "# x_train[0] = pd.to_numeric(x_train[0], errors='coerce')\n",
    "# y_train[0] = pd.to_numeric(y_train[0], errors='coerce')\n",
    "x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3db356",
   "metadata": {},
   "source": [
    "# Training our model\n",
    "1) Model Selection\n",
    "\n",
    "using the sci-kit learn cheatsheet we are gonna use the following ml models for this project.\n",
    "If one of them doesnt work we will try the other one\n",
    "\n",
    "-> Linear SVC \n",
    "-> KNeighbors Classifier\n",
    "-> SVC (Ensemble Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103372ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforemd_train_Set_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "664f1d09",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Specifying the columns using strings is only supported for pandas DataFrames",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\desktop\\MLDL projects\\env\\Lib\\site-packages\\sklearn\\utils\\__init__.py:424\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[1;34m(X, key)\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 424\u001b[0m     all_columns \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[114], line 106\u001b[0m\n\u001b[0;32m    103\u001b[0m X_train_split\u001b[38;5;241m.\u001b[39mshape,X_val_split\u001b[38;5;241m.\u001b[39mshape, y_train_split\u001b[38;5;241m.\u001b[39mshape, y_val_split\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    104\u001b[0m transformer \u001b[38;5;241m=\u001b[39m CT([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mone_hot\u001b[39m\u001b[38;5;124m\"\u001b[39m,one_hot,feature_not_numbers)],\n\u001b[0;32m    105\u001b[0m                 remainder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 106\u001b[0m X_train_encoded \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X_train_split)\n\u001b[0;32m    107\u001b[0m X_test_encoded \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(missing_val_filled_test_set_df)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# y_train_encoded = one_hot.fit_transform(ytrain_set_df_target)\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# x_train_array = X_train_encoded.toarray().astype(np.float32)\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# # y_train_array = y_train.toarray().astype(np.float32)\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# x_test_array = X_test_encoded.toarray().astype(np.float32)\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# y_train_array = np.array(ytransformed_target_train_set_df)\u001b[39;00m\n",
      "File \u001b[1;32m~\\desktop\\MLDL projects\\env\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32m~\\desktop\\MLDL projects\\env\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:724\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    722\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    723\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_transformers()\n\u001b[1;32m--> 724\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_column_callables(X)\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_remainder(X)\n\u001b[0;32m    727\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_transform(X, y, _fit_transform_one)\n",
      "File \u001b[1;32m~\\desktop\\MLDL projects\\env\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:426\u001b[0m, in \u001b[0;36mColumnTransformer._validate_column_callables\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    424\u001b[0m         columns \u001b[38;5;241m=\u001b[39m columns(X)\n\u001b[0;32m    425\u001b[0m     all_columns\u001b[38;5;241m.\u001b[39mappend(columns)\n\u001b[1;32m--> 426\u001b[0m     transformer_to_input_indices[name] \u001b[38;5;241m=\u001b[39m _get_column_indices(X, columns)\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_columns \u001b[38;5;241m=\u001b[39m all_columns\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformer_to_input_indices \u001b[38;5;241m=\u001b[39m transformer_to_input_indices\n",
      "File \u001b[1;32m~\\desktop\\MLDL projects\\env\\Lib\\site-packages\\sklearn\\utils\\__init__.py:426\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[1;34m(X, key)\u001b[0m\n\u001b[0;32m    424\u001b[0m     all_columns \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m--> 426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    427\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpecifying the columns using strings is only \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupported for pandas DataFrames\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    429\u001b[0m     )\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    431\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [key]\n",
      "\u001b[1;31mValueError\u001b[0m: Specifying the columns using strings is only supported for pandas DataFrames"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "one_hot = OneHotEncoder()\n",
    "\n",
    "# y_train_encoded = one_hot.fit_transform(y_train)\n",
    "#### for using one hot encoder we always require 2d arrays data set, it wont work on 1d array dataset\n",
    "# X_train,X_test,y_train,y_test = train_test_split(X_train_encoded,y_train,test_size=0.2)\n",
    "model = RandomForestClassifier()\n",
    "# fitted = model.fit(x_train,y_train)\n",
    "# score = fitted.score(x_test,y_train)\n",
    "# score\n",
    "########################### Dimensionality reduction\n",
    "################ PCA doesnt take sparse inputs\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# def apply_pca(data, n_components):\n",
    "#     pca = PCA(n_components=n_components)\n",
    "#     reduced_data = pca.fit_transform(data)\n",
    "#     return reduced_data\n",
    "\n",
    "# # Example usage:\n",
    "# n_components = 614  # Number of desired components\n",
    "# train_reduced_features = apply_pca(x_train, n_components)\n",
    "# test_reduced_features = apply_pca(x_test, n_components)\n",
    "\n",
    "################## Feature Extraction techniques\n",
    "########### require a pre defined feature extraction method\n",
    "# def extract_features(data):\n",
    "#     # Apply feature extraction techniques to convert data to fixed-length features\n",
    "#     extracted_features = []\n",
    "#     for datum in data:\n",
    "#         # Apply feature extraction methods such as averaging, statistical measures, etc.\n",
    "#         extracted_feature = extract_feature_from_data(datum)\n",
    "#         extracted_features.append(extracted_feature)\n",
    "#     return np.array(extracted_features)\n",
    "\n",
    "# # Example usage:\n",
    "# train_extracted_features = extract_features(X_train_encoded)\n",
    "# test_extracted_features = extract_features(X_test_encoded)\n",
    "# len(test_extracted_features)\n",
    "\n",
    "####################### Susbset selection\n",
    "############# not working\n",
    "# def select_common_features(train_features, test_features):\n",
    "#     common_features = set(train_features[0]).intersection(set(test_features[0]))\n",
    "#     train_selected_features = []\n",
    "#     test_selected_features = []\n",
    "#     for train_feature, test_feature in zip(train_features, test_features):\n",
    "#         selected_train_feature = [value for value in train_feature if value in common_features]\n",
    "#         selected_test_feature = [value for value in test_feature if value in common_features]\n",
    "#         if selected_train_feature:\n",
    "#             train_selected_features.append(selected_train_feature)\n",
    "#         else:\n",
    "#             train_selected_features.append([])\n",
    "#         if selected_test_feature:\n",
    "#             test_selected_features.append(selected_test_feature)\n",
    "#         else:\n",
    "#             test_selected_features.append([])\n",
    "#     return np.array(train_selected_features), np.array(test_selected_features)\n",
    "\n",
    "# # Example usage:\n",
    "# try:\n",
    "#     train_selected_features, test_selected_features = select_common_features(x_train, x_test)\n",
    "# except KeyError:\n",
    "#     print(\"gg brother\")\n",
    "# len(x_test)\n",
    "# len(x_train)\n",
    "# train_selected_features, test_selected_features = select_common_features(X_train_encoded, X_test_encoded)\n",
    "#################### padding last choice i guess\n",
    "# import numpy as np\n",
    "\n",
    "# def pad_features(features, max_length):\n",
    "#     padded_features = []\n",
    "#     for feature in features:\n",
    "#         if len(feature) < max_length:\n",
    "#             padded_feature = np.pad(feature, (0, max_length - len(feature)), 'constant')\n",
    "#         else:\n",
    "#             padded_feature = feature[:max_length]\n",
    "#         padded_features.append(padded_feature)\n",
    "#     return np.array(padded_features)\n",
    "\n",
    "# # Example usage:\n",
    "# max_length = 314  # Maximum length of feature vectors\n",
    "# padded_train_features = pad_features(x_train, max_length)\n",
    "# padded_test_features = pad_features(x_test, max_length)\n",
    "# len(padded_train_features),len(padded_test_features)\n",
    "# padded_test_features\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_enco = LabelEncoder()\n",
    "\n",
    "x_train = filled_train_Set\n",
    "y_train = ytransformed_target_train_set_df\n",
    "x_test = filled_test_Set\n",
    "# transformer = CT([(\"one_hot\",one_hot,feature_not_numbers)],\n",
    "#                 remainder=\"passthrough\")\n",
    "# X_train_encoded = transformer.fit_transform(missing_val_filled_train_set_df)\n",
    "# X_test_encoded = transformer.fit_transform(transforemd_test_Set_df)\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(x_train, y_train, test_size=0.4, random_state=42)\n",
    "X_train_split.shape,X_val_split.shape, y_train_split.shape, y_val_split.shape\n",
    "transformer = CT([(\"one_hot\",one_hot,feature_not_numbers)],\n",
    "                remainder=\"passthrough\")\n",
    "X_train_encoded = transformer.fit_transform(X_train_split)\n",
    "X_test_encoded = transformer.fit_transform(missing_val_filled_test_set_df)\n",
    "# y_train_encoded = one_hot.fit_transform(ytrain_set_df_target)\n",
    "# x_train_array = X_train_encoded.toarray().astype(np.float32)\n",
    "# # y_train_array = y_train.toarray().astype(np.float32)\n",
    "# x_test_array = X_test_encoded.toarray().astype(np.float32)\n",
    "# y_train_array = np.array(ytransformed_target_train_set_df)\n",
    "\n",
    "model.fit(X_train_encoded,y_train_split)\n",
    "y_test_preds = model.predict(x_test)\n",
    "\n",
    "# model.score(X_test_encoded,y_test)\n",
    "# for i in range(len(x_train_array)):\n",
    "#     if x_test_array[i] == x_train_array[i]:\n",
    "#         print(True)\n",
    "#     else:\n",
    "#         print(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b71a0a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(X_train_split,y_train_split)\n",
    "# y_test_preds = model.predict(X_test_encoded)\n",
    "# print(len(y_test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261f0dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_train[1:368])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1358febc",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_val_filled_train_set_df\n",
    "ytrain_set_df_target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed8d689",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18943cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "# Assuming you have a DataFrame with one-hot encoded features and target variable\n",
    "missing_val_filled_train_set_df['CoapplicantIncome'] = missing_val_filled_train_set_df['CoapplicantIncome'].astype(str)\n",
    "missing_val_filled_train_set_df['LoanAmount'] = missing_val_filled_train_set_df['LoanAmount'].astype(str)\n",
    "missing_val_filled_train_set_df['Loan_Amount_Term'] = missing_val_filled_train_set_df['Loan_Amount_Term'].astype(str)\n",
    "missing_val_filled_train_set_df['Credit_History'] = missing_val_filled_train_set_df['Credit_History'].astype(str)\n",
    "X_train = missing_val_filled_train_set_df\n",
    "y_train = ytrain_set_df_target\n",
    "# Create an instance of the OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Fit and transform the input features\n",
    "X_train_encoded = encoder.fit_transform(X_train)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train_encoded, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an instance of the classification algorithm\n",
    "# we will be using all three models at once\n",
    "\n",
    "model_dict = {'LinearSVC':LinearSVC(),\n",
    "              'KNeighborsClassifier':KNeighborsClassifier(),\n",
    "              'SVC':SVC()}\n",
    "# now we will create a model that will fit and score on these models\n",
    "def fit_and_Score(model, X_train_split, y_train_split, X_val_split, y_val_split):\n",
    "    model_scores_list = []\n",
    "    for key,classifier in model_dict.items():\n",
    "        model_fit = classifier.fit(X_train_split,y_train_split)\n",
    "        model_scores = model_fit.score(X_val_split,y_val_split)\n",
    "        model_scores_list.append(model_scores)\n",
    "    return model_scores_list\n",
    "\n",
    "for classifier in model_dict:\n",
    "    function_call = fit_and_Score(model = classifier,\n",
    "                                  X_train_split = X_train_split,\n",
    "                                  y_train_split = y_train_split,\n",
    "                                  X_val_split = X_val_split,\n",
    "                                  y_val_split = y_val_split)\n",
    "Final_score = function_call\n",
    "Final_score\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "# # Evaluate the model\n",
    "# accuracy = classifier.score(X_val_split, y_val_split)\n",
    "# accuracy\n",
    "# X_train_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae51cbe",
   "metadata": {},
   "source": [
    "## Now we will do cross validation of the above data, to improve the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b292efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {}\n",
    "for index,i in enumerate(Final_score):\n",
    "    result_dict.update({list(model_dict)[index]:i})\n",
    "result_dict\n",
    "################################## starting cvs\n",
    "from sklearn.model_selection import cross_val_score\n",
    "np.random.seed(42)\n",
    "# acc = cross_val_score(clf, X, y, cv=5, scoring=\"accuracy\")\n",
    "# acc_score = np.mean(acc)\n",
    "cvs_result_list = []\n",
    "for key,classifier in model_dict.items():\n",
    "    MODEL = classifier\n",
    "    scoring_acc = cross_val_score(MODEL,X_train_encoded,y_train,cv = 5, scoring=\"accuracy\")\n",
    "    cross_validated_Score = np.mean(scoring_acc)\n",
    "    cvs_result_list.append(cross_validated_Score)\n",
    "cvs_result_list\n",
    "y_preds = MODEL.predict_proba(X_test_encoded).reshape(-1,)\n",
    "# test_probs = model.predict_proba(test_features).reshape(-1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce31f5b",
   "metadata": {},
   "source": [
    "## checking various metric results like classification report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2ba8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### trying to use the fitted model on a different test dataset of different legnth as compared to the train datatset\n",
    "\n",
    "# encoder = OneHotEncoder()\n",
    "# np.random.seed(42)\n",
    "# X_train_split = encoder.fit_transform(x_train)\n",
    "# # y_train_split = encoder.fit_transform(y_train)\n",
    "# y_train_split = ytrain_set_df_target\n",
    "# X_test_split = encoder.fit_transform(missing_val_filled_test_set_df)\n",
    "# clf = KNeighborsClassifier()\n",
    "# clf = TruncatedSVD(100)\n",
    "# Xpca = clf.fit_transform(X)\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# def apply_pca(data, n_components):\n",
    "#     pca = PCA(n_components=n_components)\n",
    "#     reduced_data = pca.fit_transform(data)\n",
    "#     return reduced_data\n",
    "\n",
    "# # Example usage:\n",
    "# n_components = 367 # Number of desired components\n",
    "# train_reduced_features = apply_pca(X_train_split, n_components)\n",
    "# test_reduced_features = apply_pca(X_test_split, n_components)\n",
    "# fittedmodel = clf.fit(train_reduced_features,y_train_split)\n",
    "# # \n",
    "\n",
    "# score = fittedmodel.score(test_reduced_features,y_train_split)\n",
    "# score\n",
    "########################################## trying to make the length of the test and train dataset equal\n",
    "# from sklearn.decomposition import TruncatedSVD\n",
    "# model = KNeighborsClassifier()\n",
    "# encoder = OneHotEncoder()\n",
    "# X_train_split = encoder.fit_transform(x_train)\n",
    "# X_test_split = encoder.fit_transform(missing_val_filled_test_set_df)\n",
    "# # clf = TruncatedSVD(100)\n",
    "# # X2_train_split = clf.fit_transform(X_train_split)\n",
    "# # X2_test_split = clf.fit_transform(X_test_split)\n",
    "\n",
    "# # from sklearn.decomposition import PCA\n",
    "\n",
    "# # def apply_pca(data, n_components):\n",
    "# #     pca = PCA(n_components=n_components)\n",
    "# #     reduced_data = pca.fit_transform(data)\n",
    "# #     return reduced_data\n",
    "\n",
    "# # # Example usage:\n",
    "# # n_components = 10 # Number of desired components\n",
    "# # train_reduced_features = apply_pca(X2_train_split, n_components)\n",
    "# # test_reduced_features = apply_pca(X2_test_split, n_components)\n",
    "# fittedmodel = model.fit(X_train_split,y_train)\n",
    "# # \n",
    "# y_preds_test = fittedmodel.predict(X_test_split)\n",
    "# # len(train_reduced_features),len(test_reduced_features)\n",
    "# score = fittedmodel.score(X_test_split,y_preds_test)\n",
    "# score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed097385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(X_train_encoded,y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034194e8",
   "metadata": {},
   "source": [
    "### pos label important info\n",
    " if you have a binary classification problem where you are predicting whether an email is spam (positive class) or not spam (negative class), you can set the \"pos_label\" to 1 (spam) to calculate accuracy specifically for spam emails. This helps you understand how well your model performs in identifying spam emails while ignoring its performance on non-spam emails.\n",
    "\n",
    "It's important to note that the \"pos_label\" parameter is not universally present in all accuracy calculation functions or libraries. Its availability may depend on the specific programming language or machine learning framework you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6205d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing all the basic tools we need\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "## importing all the ml models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "## importing preprocessing tools\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "## Model Evaluating tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n",
    "from sklearn.metrics import ConfusionMatrixDisplay,confusion_matrix,classification_report\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "## setting random seed\n",
    "np.random.seed(42)\n",
    "dataset = train_set\n",
    "##experimentation with our dataset to know more about it\n",
    "\n",
    "# x = dataset[\"target\"].value_counts()\n",
    "\n",
    "## splitting our data into train and test sets\n",
    "x_train = missing_val_filled_train_set_df\n",
    "y_train2 = ytrain_set_df_target\n",
    "X_transformed = one_hot.fit_transform(x_train)\n",
    "\n",
    "X = X_transformed\n",
    "y = y_train2\n",
    "\n",
    "x_test = missing_val_filled_test_set_df\n",
    "## preprocrssing our data and handling the missing values\n",
    "one_hot = OneHotEncoder()\n",
    "\n",
    "## now we will be creating a function to use these models again and again \n",
    "# model_dict = {'LinearSVC':LinearSVC(),\n",
    "#               \"LogisticRegression\":LogisticRegression(),\n",
    "#               'KNeighborsClassifier':KNeighborsClassifier(),\n",
    "#               'SVC':SVC(),\n",
    "#               \"RandomForestClassifier\":RandomForestClassifier()}\n",
    "model_dict = {'LinearSVC':LinearSVC(),\n",
    "              'KNeighborsClassifier':KNeighborsClassifier(),\n",
    "              'SVC':SVC()\n",
    "              }\n",
    "accuracy_dict = {}\n",
    "precision_dict = {}\n",
    "recall_dict = {}\n",
    "scoring_metrics = [\"accuracy\",\"precision\",\"recall\"]\n",
    "def model_function(model,X_test,X_train,y_test,y_train):\n",
    "    for key,classifier in model_dict.items():\n",
    "        model_fit = classifier.fit(X_train,y_train)\n",
    "        model_score = model_fit.score(X_test,y_test)\n",
    "## Scoring using cross validation to get better results on accuracy,precision,recall use the following code\n",
    "## instead of the code not commented\n",
    "#     np.random.seed(42)\n",
    "#     scoring_metrics = [\"accuracy\",\"precision\",\"recall\"]\n",
    "#     for key,classifier in model_dict.items():\n",
    "#         model_fit = classifier.fit(X_train,y_train)\n",
    "#         for metric in scoring_metrics:\n",
    "#             model_cvs = cross_val_score(classifier,X,y,cv=5,scoring=metric)\n",
    "#             model_score = np.mean(model_cvs)\n",
    "#             print(f'Model:{key} -- {metric} Score = {model_score}')\n",
    "        accuracy_dict[key] = model_score\n",
    "        y_preds = classifier.predict(X_test)\n",
    "        \n",
    "        \n",
    "        \n",
    "        precision_dict[key] = precision_score(y_true=y_test, y_pred=y_preds, pos_label=\"Y\")\n",
    "        recall_dict[key] = recall_score(y_true=y_test, y_pred=y_preds, pos_label=\"Y\")\n",
    "        model_compare_acc = pd.DataFrame(accuracy_dict,index = [\"accuracy\"])\n",
    "        model_compare_pre = pd.DataFrame(precision_dict,index = [\"precision\"])       \n",
    "        model_compare_rec = pd.DataFrame(recall_dict,index = [\"recall_Score\"])\n",
    "\n",
    "    yield model_compare_acc\n",
    "    yield model_compare_pre\n",
    "    yield model_compare_rec\n",
    "    ## keeping extra yields in case if we require more models to test\n",
    "    yield \"\"\n",
    "    yield \"\"\n",
    "    yield \"\"\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "for classifier in model_dict:\n",
    "    method_call = model_function(model = classifier,\n",
    "                                 X_test = X_test,\n",
    "                                 X_train = X_train,\n",
    "                                 y_test = y_test,\n",
    "                                 y_train = y_train)\n",
    "\n",
    "for lim in range(len(model_dict)):\n",
    "    print(method_call.__next__())\n",
    "# plotting our results\n",
    "# use the below commented code in case if there is error in plotting graphs\n",
    "model_compare_acc = pd.DataFrame(accuracy_dict,index = [\"accuracy\"])\n",
    "model_compare_pre = pd.DataFrame(precision_dict,index = [\"precision\"])       \n",
    "model_compare_rec = pd.DataFrame(recall_dict,index = [\"recall_Score\"])\n",
    "print(\"This Result is on a single test set without using corss validation\")\n",
    "##### Use the below code to print al the metrics ourside of the function\n",
    "# print(f'{model_compare_acc}\\n {model_compare_pre}\\n {model_compare_rec}')\n",
    "acc_plot = model_compare_acc.T.plot.bar()\n",
    "pre_plot = model_compare_pre.T.plot.bar()\n",
    "recall_plot = model_compare_rec.T.plot.bar()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880288ce",
   "metadata": {},
   "source": [
    "## Result Analysis:\n",
    "from the above result we can see that models, Logistic Regression, SVC, and RFC are giving the same results hence we will be discarding them also SVC is giving better results as compared to LinearSVC and KneighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a91fa9",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "to improve SVC model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cb6eb7",
   "metadata": {},
   "source": [
    "## 1) cross validation for svc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2953f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "svc = SVC()\n",
    "fittedmodel = svc.fit(X,y)\n",
    "scoring_metrics = [\"accuracy\",\"roc_auc\",\"f1\"]\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# # Restore warnings\n",
    "# warnings.filterwarnings(\"default\")\n",
    "for metric in scoring_metrics:\n",
    "    cvs = cross_val_score(svc, X, y, cv=5, scoring=metric, verbose=1)\n",
    "    cross_validated_score = np.mean(cvs)\n",
    "    print(f'Model:SVC -- {metric} Score = {cross_validated_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfec3b8",
   "metadata": {},
   "source": [
    "## 2) Random Search Cross Vaildation(RSCV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7626b690",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameter tuning using RSCV\n",
    "# Creating a HP grid for SVC\n",
    "np.random.seed(42)\n",
    "SVC_grid = {\"C\":np.logspace(-4, 4 ,20),\n",
    "            \"break_ties\":[5,5,10]}\n",
    "rs_svc = RandomizedSearchCV(SVC(),\n",
    "                           param_distributions=SVC_grid,\n",
    "                          cv=5,\n",
    "                          n_iter=20,\n",
    "                          verbose=True)\n",
    "rs_svc.fit(X_train,y_train)\n",
    "rs_svc.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee685de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### using these params in above model fitting\n",
    "import warnings\n",
    "svc = SVC(break_ties=5, C=1.623776739188721)\n",
    "fittedmodel = svc.fit(X,y)\n",
    "scoring_metrics = [\"accuracy\",\"roc_auc\",\"f1\"]\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# # Restore warnings\n",
    "# warnings.filterwarnings(\"default\")\n",
    "for metric in scoring_metrics:\n",
    "    cvs = cross_val_score(svc, X, y, cv=5, scoring=metric, verbose=1)\n",
    "    cross_validated_score = np.mean(cvs)\n",
    "    print(f'Model:SVC -- {metric} Score = {cross_validated_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a119c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6d8af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "classifier = SVC()\n",
    "model_fit = classifier.fit(X_train,y_train)\n",
    "\n",
    "y_preds = classifier.predict(X_test)  \n",
    "\n",
    "# ## Plot ROC Curve and finding the AUC metric\n",
    "# # AUC metric is generally written below the graph\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# # Compute ROC curve\n",
    "# fpr, tpr, thresholds = roc_auc_score(y_test, y_preds)\n",
    "\n",
    "# # Plot ROC curve\n",
    "# plt.plot(fpr, tpr)\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('ROC Curve')\n",
    "# plt.show()\n",
    "## Confusion matrix\n",
    "# seaborn heat map\n",
    "sns.set(font_scale=1.5) # Increase font size\n",
    " \n",
    "def plot_conf_mat(y_test, y_preds):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix using Seaborn's heatmap().\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(3, 3))\n",
    "    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n",
    "                     annot=True, # Annotate the boxes\n",
    "                     cbar=False)\n",
    "    plt.xlabel(\"Predicted label\") # predictions go on the x-axis\n",
    "    plt.ylabel(\"True label\") # true labels go on the y-axis \n",
    "  \n",
    "plot_conf_mat(y_test, y_preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689faae8",
   "metadata": {},
   "source": [
    "# Approaching the problem using regression\n",
    "on Loan_Amount vs other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32569b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We already got the tranformed and missing values filled dataset hence we would just start training our model\n",
    "X_reg = X\n",
    "y_reg = missing_val_filled_train_set_df[\"LoanAmount\"]\n",
    "y_reg_transformed = label_enco.fit_transform(y_reg)\n",
    "np.random.seed(42)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "model_dcit_reg = {\"RandomForestRegressor\": RandomForestRegressor(),\n",
    "                  \"Ridge\": Ridge(),\n",
    "                  \"Lasso\": Lasso()\n",
    "                 }\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_reg,y_reg_transformed,test_size=0.2)\n",
    "\n",
    "for key,model in model_dcit_reg.items():\n",
    "    fitted_model = model.fit(X_train,y_train)\n",
    "    score = fitted_model.score(X_test,y_test)\n",
    "    print(f'{key} score = {score}')\n",
    "    \n",
    "#Lasso Regression\n",
    "\n",
    "\n",
    "\n",
    "# #Initializing the Lasso Regressor with Normalization Factor as True\n",
    "# lasso_reg = Lasso(normalize=True)\n",
    "# #Fitting the Training data to the Lasso regressor\n",
    "# lasso_reg.fit(X_train,Y_train)\n",
    "# #Predicting for X_test\n",
    "# y_pred_lass =lasso_reg.predict(X_test)\n",
    "# #Printing the Score with RMLSE\n",
    "# print(\"\\n\\nLasso SCORE : \", score(y_pred_lass, actual_cost))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ea7587",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d05c98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "np.random.seed(42)\n",
    "scoring_metrics_reg  = [\"r2\"]\n",
    "for key,model in model_dcit_reg.items():\n",
    "    fitted_model = model.fit(X_train,y_train)\n",
    "    for metric in scoring_metrics_reg:\n",
    "        score = cross_val_score(fitted_model, X_reg, y_reg_transformed, cv=5, scoring=metric)\n",
    "        resulted_score = np.mean(score)\n",
    "        print(f'{key}:{metric} = {resulted_score}')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1e8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import get_scorer_names\n",
    "get_scorer_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea47f4be",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f4e2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### will try to find the best parameters for Random forest Regressor and Ridge Regressor to improve our accuracy\n",
    "## Hyperparameter tuning using RSCV\n",
    "# Creating a HP grid for RFR \n",
    "# np.random.seed(42)\n",
    "# RFR_grid = {\n",
    "#             'bootstrap': [True, False],\n",
    "#             'max_depth': [20, 40, 60, 80, 100, None],\n",
    "#             'max_features': ['auto'],\n",
    "#             'min_samples_leaf': [1, 2, 4],\n",
    "#             'min_samples_split': [2, 5, 10],\n",
    "#             'n_estimators': [100,200]}\n",
    "# rs_rfr = RandomizedSearchCV(RandomForestRegressor(),\n",
    "#                             param_distributions=RFR_grid,\n",
    "#                             cv=5,\n",
    "#                             n_iter=20,\n",
    "#                             verbose=True)\n",
    "# rs_rfr.fit(X_train,y_train)\n",
    "\n",
    "# rs_rfr.best_params_\n",
    "############ for Lasso reg\n",
    "Lasso_grid = {\n",
    "            'alpha':[0.5,1],\n",
    "    'fit_intercept':[True,False],\n",
    "    'precompute':[True,False],\n",
    "    'copy_X':[True,False],\n",
    "    'max_iter':[50,100,200]\n",
    "        }\n",
    "rs_lasso = RandomizedSearchCV(Lasso(),\n",
    "                            param_distributions=Lasso_grid,\n",
    "                            cv=5,\n",
    "                            n_iter=20,\n",
    "                            verbose=True)\n",
    "rs_lasso.fit(X_train,y_train)\n",
    "\n",
    "rs_lasso.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a56d7a2",
   "metadata": {},
   "source": [
    "#### These are the best parameters that we got  for HP tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f260e761",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Lasso' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[81], line 11\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error\n\u001b[0;32m      4\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      5\u001b[0m model_dict_reg \u001b[38;5;241m=\u001b[39m  {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandomForestRegressor\u001b[39m\u001b[38;5;124m\"\u001b[39m: RandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,\n\u001b[0;32m      6\u001b[0m                                                                   min_samples_leaf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m      7\u001b[0m                                                                   min_samples_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m      8\u001b[0m                                                                   max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      9\u001b[0m                                                                   bootstrap\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     10\u001b[0m                                                                   max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m),\n\u001b[1;32m---> 11\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLasso\u001b[39m\u001b[38;5;124m\"\u001b[39m: Lasso(precompute\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     12\u001b[0m                                  max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[0;32m     13\u001b[0m                                  fit_intercept\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     14\u001b[0m                                  copy_X\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     15\u001b[0m                                  alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)}\n\u001b[0;32m     16\u001b[0m scoring_metrics_reg  \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr2\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key,model \u001b[38;5;129;01min\u001b[39;00m model_dcit_reg\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Lasso' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "np.random.seed(42)\n",
    "model_dict_reg =  {\"RandomForestRegressor\": RandomForestRegressor(n_estimators=500,\n",
    "                                                                  min_samples_leaf=2,\n",
    "                                                                  min_samples_split=5,\n",
    "                                                                  max_features='auto',\n",
    "                                                                  bootstrap=True,\n",
    "                                                                  max_depth=20),\n",
    "                   \"Lasso\": Lasso(precompute=True,\n",
    "                                 max_iter=50,\n",
    "                                 fit_intercept=True,\n",
    "                                 copy_X=False,\n",
    "                                 alpha=0.5)}\n",
    "scoring_metrics_reg  = [\"r2\"]\n",
    "for key,model in model_dcit_reg.items():\n",
    "    fitted_model = model.fit(X_train,y_train)\n",
    "    for metric in scoring_metrics_reg:\n",
    "        score = cross_val_score(fitted_model, X_reg, y_reg_transformed, cv=15, scoring=metric)\n",
    "        resulted_score = np.mean(score)\n",
    "        print(f'{key}:{metric} = {resulted_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e8bb2e",
   "metadata": {},
   "source": [
    "#### Hence these are our final regression r2 scores for Loan Eligibilty predicter problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5753e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
