{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7362783",
   "metadata": {},
   "source": [
    "# Problem Definition\n",
    "Creating a model that will first find out wether a person is eligible for loan or not and if eligible than how much loan the user can obtain based on various factors such as the userâ€™s income, education, etc.\n",
    "## Data Source\n",
    "https://www.kaggle.com/datasets/altruistdelhite04/loan-prediction-problem-dataset\n",
    "## Conditions that must match for this model\n",
    "First of all, one cannot predict loan amount using regression models and 981 instances (combining training and testing set) with R2 Score greater than 0.44\n",
    "\n",
    "Secondly, the dateset contains many missing values. So even if using imputer with different parameters for categorical and non-categorical data, ultimately data-set loses variability to some extent.\n",
    "\n",
    "You do imputation, label encoding, one-hot encoding and standardization, and even hyper-parameters tuning in an effort to increase accuracy. But the performance benchmarks around R2 Score of 0.44\n",
    "\n",
    "If still want to try to improve accuracy, you test different combinations of features. But still you get no significant improvement gains\n",
    "\n",
    "Leaving regressors aside now and testing classification to somehow improve accuracy by categorizing loan amount by defining ranges, you get to see accuracy jumping to ~0.7 and F1 Score to ~0.6 with all the optimization techniques applied above.\n",
    "\n",
    "It's tested and proven with all available ML models. So to better predict loan amount better increase the number of instances to more than 5000!\n",
    "\n",
    "These are the some conditions that must match in this model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "da857da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the required modules for the mcahine learning project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13085c0d",
   "metadata": {},
   "source": [
    "## Getting the Data ready for splitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da843533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we dont require to split data as it is already splitted for us \n",
    "#data = pd.read_excel(\"LoanPredData/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba30b62",
   "metadata": {},
   "source": [
    "## Accessing the data\n",
    "Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "912a5d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using readcsv as the data we have in exel is in the csv format\n",
    "train_set = pd.read_csv(\"LoanPredData/train_u6lujuX_CVtuZ9i.csv\")\n",
    "train_set_df = pd.DataFrame(train_set)\n",
    "xtrain_set_df_features = train_set_df.drop(\"Loan_Status\",axis=1)\n",
    "ytrain_set_df_target = train_set[\"Loan_Status\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1cb119",
   "metadata": {},
   "source": [
    "## Accesing the data\n",
    "Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f29fc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Set = pd.read_csv(\"LoanPredData/test_Y3wMUE5_7gLdaTN.csv\")\n",
    "xtest_Set_df = pd.DataFrame(test_Set)\n",
    "xtest_Set_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58acff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of the data we have\n",
    "len(train_set_df),len(xtest_Set_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ad8f52",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis( EDA)\n",
    "1) What are we trying to solve\n",
    "\n",
    "We are trying to create a ml model which will tell us how much loan the user can obtain using various attributes\n",
    "\n",
    "2) Type of Data we have\n",
    "\n",
    "We got the data both in numbers and strings also integers and strings mixed\n",
    "\n",
    "3) Missing Values\n",
    "\n",
    "We will be checking that int the code below\n",
    "\n",
    "4) What are the outliers(Random samples too different from other or not)\n",
    "\n",
    "Outliers are those data points that are significantly different from the rest of the dataset. They are often abnormal observations that skew the data distribution, and arise due to inconsistent data entry, or erroneous observations, will check that also\n",
    "\n",
    "5) Add change or remove features to get the most out of our data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c740221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values and yes we have too many of them\n",
    "train_set.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f251c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for the data types of each columns\n",
    "type_testing_list = []\n",
    "for key,value in enumerate(train_set_df):\n",
    "    x = type(train_set_df[value][6])\n",
    "    type_testing_list.append(x)\n",
    "type_testing_list\n",
    "\n",
    "# # (other method)code for checking the length,dtype of each column\n",
    "# for key,values in train_set_df.iteritems():\n",
    "#     print(key,values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e667fc",
   "metadata": {},
   "source": [
    "### Now we will be checking wether the user is eligible for loan or not ,using classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6b631f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first finding out how many of classes are there on our target variable\n",
    "loan_status = train_set_df.value_counts(\"Loan_Status\")\n",
    "loan_status.plot(kind=\"bar\",color=[\"Green\",\"Blue\"])\n",
    "loan_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9aff40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_set_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c44b30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e86f12",
   "metadata": {},
   "source": [
    "### Now we will be doing data study, like on what factors our model dependa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5bd01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender = train_set_df.value_counts(\"Gender\")\n",
    "gender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9205c8a9",
   "metadata": {},
   "source": [
    "### comparing how gender affects the target variable loan status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452564a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.crosstab(train_set_df[\"Gender\"],train_set_df[\"Loan_Status\"])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c6ba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.plot(kind=\"bar\",color=[\"salmon\",\"blue\"],figsize=(4,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb395e6",
   "metadata": {},
   "source": [
    "Damn, i guess we will have to first handle missing values\n",
    "\n",
    "les go then,\n",
    "### now we have two options \n",
    "first to remove all the rows with missing values but this can result in data loss and not working of the model correctly\n",
    "second is to replace the missing values, lets try the first method \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7035fc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set_df.dropna(subset=[\"Gender\",\"Married\",\"Dependents\",\"Self_Employed\",\"LoanAmount\",\"Loan_Amount_Term\",\n",
    "#                            \"Credit_History\"], inplace=True)\n",
    "train_set_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd112499",
   "metadata": {},
   "source": [
    "lmao there are no missing values in the target column,mb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94029798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we can see the data loss is too much, hence we will use the second option that is to \n",
    "# be rplacing values  now using imputer\n",
    "len(train_set_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3823e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## replacing missing values using imputer\n",
    "from sklearn.impute import SimpleImputer as SI\n",
    "from sklearn.compose import ColumnTransformer as CT\n",
    "# now here we will be filling string values with missing and integer values with mean\n",
    "typ1 = SI(strategy=\"constant\", fill_value=\"missing\")\n",
    "# Married_feature = SI(strategy=\"constant\", fill_value=\"missing\")\n",
    "typ2 = SI(strategy=\"constant\", fill_value=\"1\")\n",
    "# Self_emp_feature = SI(strategy=\"constant\", fill_value=\"missing\")\n",
    "typ3 = SI(strategy=\"mean\")\n",
    "col1 = [\"Loan_ID\",\"Gender\",\"Married\",\"Self_Employed\",\"ApplicantIncome\",\"CoapplicantIncome\",\"Property_Area\"]\n",
    "col2 = [\"Dependents\",\"Education\",\"Credit_History\"]\n",
    "col3 = [\"LoanAmount\",\"Loan_Amount_Term\"]\n",
    "# creating an imputer , SOmething that will fill on the missing data\n",
    "imputer = CT([(\"typ1\",typ1,col1),\n",
    "              (\"typ2\",typ2,col2),\n",
    "              (\"typ3\",typ3,col3)])\n",
    "\n",
    "# filled training set\n",
    "filled_train_Set = imputer.fit_transform(xtrain_set_df_features)\n",
    "len(filled_train_Set)\n",
    "\n",
    "##filling the new data into a new dataframe \n",
    "missing_val_filled_train_set_df = pd.DataFrame(filled_train_Set,\n",
    "                                              columns=[\"Loan_ID\",\"Gender\",\"Married\",\"Self_Employed\",\"ApplicantIncome\",\n",
    "                                                       \"CoapplicantIncome\",\"Property_Area\",\n",
    "                                                       \"Dependents\",\"Education\",\"Credit_History\",\"LoanAmount\",\n",
    "                                                       \"Loan_Amount_Term\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93a53f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOW LETS CHECK WETHER WE FILLED THE MISSING VALUES OR NOT\n",
    "missing_val_filled_train_set_df.isna().sum()\n",
    "# train_set_df\n",
    "# filled_train_Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8700d62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YES WE FILLED IT,  LETS CHECK THE LENGTH OF THE DATASET TO CHECK FOR DATA LOSS\n",
    "len(missing_val_filled_train_set_df)\n",
    "# NO DATA LOSS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d0d43d",
   "metadata": {},
   "source": [
    "## Doing the data conversion from other dtypes to int\n",
    "so that it would be easier for our machine to work on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cd9b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_val_filled_train_set_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "58200569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0, 0)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 617)\\t1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0, 1)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0, 2)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(0, 3)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0, 4)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 617)\\t1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>(0, 609)\\t1.0\\n  (0, 614)\\t1.0\\n  (0, 617)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>(0, 610)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>(0, 611)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>(0, 612)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>(0, 613)\\t1.0\\n  (0, 614)\\t1.0\\n  (0, 617)\\t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>614 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0\n",
       "0      (0, 0)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 617)\\t1....\n",
       "1      (0, 1)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t1....\n",
       "2      (0, 2)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t1....\n",
       "3      (0, 3)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t1....\n",
       "4      (0, 4)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 617)\\t1....\n",
       "..                                                 ...\n",
       "609    (0, 609)\\t1.0\\n  (0, 614)\\t1.0\\n  (0, 617)\\t...\n",
       "610    (0, 610)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t...\n",
       "611    (0, 611)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t...\n",
       "612    (0, 612)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t...\n",
       "613    (0, 613)\\t1.0\\n  (0, 614)\\t1.0\\n  (0, 617)\\t...\n",
       "\n",
       "[614 rows x 1 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## .astype wont work as data has missing values such as NaN\n",
    "#train_set_df.astype(\"int\")\n",
    "## hence we will be usng one hot encoder, to encode all those values\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "#  0   Loan_ID            614 non-null    object \n",
    "#  1   Gender             601 non-null    object \n",
    "#  2   Married            611 non-null    object \n",
    "#  3   Dependents         599 non-null    object \n",
    "#  4   Education          614 non-null    object \n",
    "#  5   Self_Employed      582 non-null    object \n",
    "#  6   ApplicantIncome    614 non-null    int64  \n",
    "#  7   CoapplicantIncome  614 non-null    float64\n",
    "#  8   LoanAmount         592 non-null    float64\n",
    "#  9   Loan_Amount_Term   600 non-null    float64\n",
    "#  10  Credit_History     564 non-null    float64\n",
    "#  11  Property_Area      614 non-null    object \n",
    "#  12  Loan_Status        614 non-null    object\n",
    "missing_val_filled_train_set_df['CoapplicantIncome'] = missing_val_filled_train_set_df['CoapplicantIncome'].astype(str)\n",
    "missing_val_filled_train_set_df['LoanAmount'] = missing_val_filled_train_set_df['LoanAmount'].astype(str)\n",
    "missing_val_filled_train_set_df['Loan_Amount_Term'] = missing_val_filled_train_set_df['Loan_Amount_Term'].astype(str)\n",
    "missing_val_filled_train_set_df['Credit_History'] = missing_val_filled_train_set_df['Credit_History'].astype(str)\n",
    "\n",
    "\n",
    "feature_not_numbers = [\"Loan_ID\",\n",
    "                        \"Gender\",\n",
    "                        \"Married\",\n",
    "                        \"Dependents\",\n",
    "                        \"Education\",\n",
    "                        \"Self_Employed\",\n",
    "                        \"CoapplicantIncome\",\n",
    "                        \"LoanAmount\",\n",
    "                        \"Loan_Amount_Term\",\n",
    "                        \"Credit_History\",\n",
    "                        \"Property_Area\",\n",
    "                        ]\n",
    "one_hot = OneHotEncoder()\n",
    "transformer = CT([(\"one_hot\",one_hot,feature_not_numbers)],\n",
    "                remainder=\"passthrough\")\n",
    "transforemd_data = transformer.fit_transform(missing_val_filled_train_set_df)\n",
    "transforemd_train_Set_df = pd.DataFrame(transforemd_data)\n",
    "\n",
    "#### these are our final transformed features from the training set\n",
    "transforemd_train_Set_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6579499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df = pd.DataFrame(ytrain_set_df_target)\n",
    "y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4f597345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>614 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1\n",
       "0    0.0  1.0\n",
       "1    1.0  0.0\n",
       "2    0.0  1.0\n",
       "3    0.0  1.0\n",
       "4    0.0  1.0\n",
       "..   ...  ...\n",
       "609  0.0  1.0\n",
       "610  0.0  1.0\n",
       "611  0.0  1.0\n",
       "612  0.0  1.0\n",
       "613  1.0  0.0\n",
       "\n",
       "[614 rows x 2 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_not_number = [\"Loan_Status\"]\n",
    "# transformer2 = CT([(\"one_hot\",one_hot,target_not_number)],\n",
    "#                  remainder=\"passthrough\")\n",
    "# ytransformed_target = transformer2.fit_transform(y_df)\n",
    "# \n",
    "\n",
    "#### This is our final transformed target from our training set\n",
    "# ytransformed_target_train_set_df\n",
    "transformer = CT([(\"one_hot\",one_hot,target_not_number)],\n",
    "                remainder=\"passthrough\")\n",
    "ytransformed_target = transformer.fit_transform(y_df)\n",
    "ytransformed_target_train_set_df = pd.DataFrame(ytransformed_target)\n",
    "ytransformed_target_train_set_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbc9776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(missing_val_filled_train_set_df.ApplicantIncome[missing_val_filled_train_set_df.Loan_Status==\"Y\"],\n",
    "#             missing_val_filled_train_set_df.Dependents[missing_val_filled_train_set_df.Loan_Status==\"Y\"],\n",
    "#       #      train_set_df.Education[train_set_df.Loan_Status==\"Y\"],\n",
    "#            c=\"salmon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483e1ada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f9b81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(missing_val_filled_train_set_df.ApplicantIncome[missing_val_filled_train_set_df.Loan_Status==\"N\"],\n",
    "#             missing_val_filled_train_set_df.Dependents[missing_val_filled_train_set_df.Loan_Status==\"N\"],\n",
    "#       #      train_set_df.Education[train_set_df.Loan_Status==\"Y\"],\n",
    "#            c=\"salmon\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cdb588",
   "metadata": {},
   "source": [
    "# Checking progress\n",
    "\n",
    " 1) Data was successfully loaded\n",
    "2) Handled the missing values\n",
    "3) data converted to numerical format for the machine to understand easily\n",
    "4) EDA was done on various attributes\n",
    "\n",
    "# Now we will start evaluating our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a8d760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lmao ded , we need to make the test dataset also work ready, damn\n",
    "xtest_Set_df.isna().sum()\n",
    "xtest_Set_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7a075a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "## replacing missing values using imputer\n",
    "from sklearn.impute import SimpleImputer as SI\n",
    "from sklearn.compose import ColumnTransformer as CT\n",
    "# now here we will be filling string values with missing and integer values with mean\n",
    "typ1 = SI(strategy=\"constant\", fill_value=\"missing\")\n",
    "# Married_feature = SI(strategy=\"constant\", fill_value=\"missing\")\n",
    "typ2 = SI(strategy=\"constant\", fill_value=\"1\")\n",
    "# Self_emp_feature = SI(strategy=\"constant\", fill_value=\"missing\")\n",
    "typ3 = SI(strategy=\"mean\")\n",
    "col1 = [\"Loan_ID\",\"Gender\",\"Married\",\"Self_Employed\",\"ApplicantIncome\",\"CoapplicantIncome\",\"Property_Area\"]\n",
    "col2 = [\"Dependents\",\"Education\",\"Credit_History\"]\n",
    "col3 = [\"LoanAmount\",\"Loan_Amount_Term\"]\n",
    "# creating an imputer , SOmething that will fill on the missing d\"ata\n",
    "imputer = CT([(\"typ1\",typ1,col1),\n",
    "              (\"typ2\",typ2,col2),\n",
    "              (\"typ3\",typ3,col3)])\n",
    "\n",
    "# filled training set\n",
    "filled_test_Set = imputer.fit_transform(xtest_Set_df)\n",
    "len(filled_test_Set)\n",
    "\n",
    "##filling the new data into a new dataframe \n",
    "missing_val_filled_test_set_df = pd.DataFrame(filled_test_Set,\n",
    "                                              columns=[\"Loan_ID\",\"Gender\",\"Married\",\"Self_Employed\",\"ApplicantIncome\",\n",
    "                                                       \"CoapplicantIncome\",\"Property_Area\",\n",
    "                                                       \"Dependents\",\"Education\",\"Credit_History\",\"LoanAmount\",\n",
    "                                                       \"Loan_Amount_Term\"])\n",
    "missing_val_filled_test_set_df\n",
    "\n",
    "missing_val_filled_test_set_df['CoapplicantIncome'] = missing_val_filled_test_set_df['CoapplicantIncome'].astype(str)\n",
    "missing_val_filled_test_set_df['LoanAmount'] = missing_val_filled_test_set_df['LoanAmount'].astype(str)\n",
    "missing_val_filled_test_set_df['Loan_Amount_Term'] = missing_val_filled_test_set_df['Loan_Amount_Term'].astype(str)\n",
    "missing_val_filled_test_set_df['Credit_History'] = missing_val_filled_test_set_df['Credit_History'].astype(str)\n",
    "########## converting to numerical format using one hot encoding\n",
    "feature_not_numbers = [\"Loan_ID\",\n",
    "                        \"Gender\",\n",
    "                        \"Married\",\n",
    "                        \"Dependents\",\n",
    "                        \"Education\",\n",
    "                        \"Self_Employed\",\n",
    "                        \"CoapplicantIncome\",\n",
    "                        \"LoanAmount\",\n",
    "                        \"Loan_Amount_Term\",\n",
    "                        \"Credit_History\",\n",
    "                        \"Property_Area\",\n",
    "                        ]\n",
    "one_hot = OneHotEncoder()\n",
    "transformer = CT([(\"one_hot\",one_hot,feature_not_numbers)],\n",
    "                remainder=\"passthrough\")\n",
    "transforemd_test_data = transformer.fit_transform(missing_val_filled_test_set_df)\n",
    "transforemd_test_Set_df = pd.DataFrame(transforemd_data)\n",
    "\n",
    "#### these are our final transformed features from the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "485d8b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0, 0)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 617)\\t1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0, 1)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0, 2)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(0, 3)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0, 4)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 617)\\t1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>(0, 609)\\t1.0\\n  (0, 614)\\t1.0\\n  (0, 617)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>(0, 610)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>(0, 611)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>(0, 612)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>(0, 613)\\t1.0\\n  (0, 614)\\t1.0\\n  (0, 617)\\t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>614 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0\n",
       "0      (0, 0)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 617)\\t1....\n",
       "1      (0, 1)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t1....\n",
       "2      (0, 2)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t1....\n",
       "3      (0, 3)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t1....\n",
       "4      (0, 4)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 617)\\t1....\n",
       "..                                                 ...\n",
       "609    (0, 609)\\t1.0\\n  (0, 614)\\t1.0\\n  (0, 617)\\t...\n",
       "610    (0, 610)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t...\n",
       "611    (0, 611)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t...\n",
       "612    (0, 612)\\t1.0\\n  (0, 615)\\t1.0\\n  (0, 618)\\t...\n",
       "613    (0, 613)\\t1.0\\n  (0, 614)\\t1.0\\n  (0, 617)\\t...\n",
       "\n",
       "[614 rows x 1 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = transforemd_train_Set_df\n",
    "y_train = ytransformed_target_train_set_df\n",
    "x_test = transforemd_test_Set_df\n",
    "# len(transforemd_train_Set_df)\n",
    "\n",
    "# x_train_array = x_train.toarray().astype(np.float32)\n",
    "# y_train_array = y_train.toarray().astype(np.float32)\n",
    "# we also need to find the values of y_test to complete this code\n",
    "# x_train[0] = pd.to_numeric(x_train[0], errors='coerce')\n",
    "# y_train[0] = pd.to_numeric(y_train[0], errors='coerce')\n",
    "x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3db356",
   "metadata": {},
   "source": [
    "# Training our model\n",
    "1) Model Selection\n",
    "\n",
    "using the sci-kit learn cheatsheet we are gonna use the following ml models for this project.\n",
    "If one of them doesnt work we will try the other one\n",
    "\n",
    "-> Linear SVC \n",
    "-> KNeighbors Classifier\n",
    "-> SVC (Ensemble Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103372ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforemd_train_Set_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "664f1d09",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'any'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 106\u001b[0m\n\u001b[0;32m     99\u001b[0m X_train_split\u001b[38;5;241m.\u001b[39mshape,X_val_split\u001b[38;5;241m.\u001b[39mshape, y_train_split\u001b[38;5;241m.\u001b[39mshape, y_val_split\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# y_train_encoded = one_hot.fit_transform(ytrain_set_df_target)\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# x_train_array = X_train_encoded.toarray().astype(np.float32)\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# # y_train_array = y_train.toarray().astype(np.float32)\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# x_test_array = X_test_encoded.toarray().astype(np.float32)\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# y_train_array = np.array(ytransformed_target_train_set_df)\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train_split,y_train_split)\n\u001b[0;32m    107\u001b[0m y_test_preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(transforemd_test_Set_df)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mlen\u001b[39m(X_train_encoded)\n",
      "File \u001b[1;32m~\\desktop\\MLDL projects\\env\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:345\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 345\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    346\u001b[0m     X, y, multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mDTYPE\n\u001b[0;32m    347\u001b[0m )\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    349\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[1;32m~\\desktop\\MLDL projects\\env\\Lib\\site-packages\\sklearn\\base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 584\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    585\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\desktop\\MLDL projects\\env\\Lib\\site-packages\\sklearn\\utils\\validation.py:1122\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1103\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[0;32m   1106\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1107\u001b[0m     X,\n\u001b[0;32m   1108\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1119\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1120\u001b[0m )\n\u001b[1;32m-> 1122\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1124\u001b[0m check_consistent_length(X, y)\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32m~\\desktop\\MLDL projects\\env\\Lib\\site-packages\\sklearn\\utils\\validation.py:1132\u001b[0m, in \u001b[0;36m_check_y\u001b[1;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1130\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Isolated part of check_X_y dedicated to y validation\"\"\"\u001b[39;00m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multi_output:\n\u001b[1;32m-> 1132\u001b[0m     y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1133\u001b[0m         y,\n\u001b[0;32m   1134\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1135\u001b[0m         force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1136\u001b[0m         ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1137\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1138\u001b[0m         input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1139\u001b[0m         estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m   1140\u001b[0m     )\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1142\u001b[0m     estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n",
      "File \u001b[1;32m~\\desktop\\MLDL projects\\env\\Lib\\site-packages\\sklearn\\utils\\validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    915\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    916\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    917\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    918\u001b[0m         )\n\u001b[0;32m    920\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 921\u001b[0m         _assert_all_finite(\n\u001b[0;32m    922\u001b[0m             array,\n\u001b[0;32m    923\u001b[0m             input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    924\u001b[0m             estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    925\u001b[0m             allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    926\u001b[0m         )\n\u001b[0;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    929\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32m~\\desktop\\MLDL projects\\env\\Lib\\site-packages\\sklearn\\utils\\validation.py:110\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nan:\n\u001b[1;32m--> 110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _object_dtype_isnan(X)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput contains NaN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# We need only consider float arrays, hence can early return for all else.\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'bool' object has no attribute 'any'"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "one_hot = OneHotEncoder()\n",
    "\n",
    "# y_train_encoded = one_hot.fit_transform(y_train)\n",
    "#### for using one hot encoder we always require 2d arrays data set, it wont work on 1d array dataset\n",
    "# X_train,X_test,y_train,y_test = train_test_split(X_train_encoded,y_train,test_size=0.2)\n",
    "model = RandomForestClassifier()\n",
    "# fitted = model.fit(x_train,y_train)\n",
    "# score = fitted.score(x_test,y_train)\n",
    "# score\n",
    "########################### Dimensionality reduction\n",
    "################ PCA doesnt take sparse inputs\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# def apply_pca(data, n_components):\n",
    "#     pca = PCA(n_components=n_components)\n",
    "#     reduced_data = pca.fit_transform(data)\n",
    "#     return reduced_data\n",
    "\n",
    "# # Example usage:\n",
    "# n_components = 614  # Number of desired components\n",
    "# train_reduced_features = apply_pca(x_train, n_components)\n",
    "# test_reduced_features = apply_pca(x_test, n_components)\n",
    "\n",
    "################## Feature Extraction techniques\n",
    "########### require a pre defined feature extraction method\n",
    "# def extract_features(data):\n",
    "#     # Apply feature extraction techniques to convert data to fixed-length features\n",
    "#     extracted_features = []\n",
    "#     for datum in data:\n",
    "#         # Apply feature extraction methods such as averaging, statistical measures, etc.\n",
    "#         extracted_feature = extract_feature_from_data(datum)\n",
    "#         extracted_features.append(extracted_feature)\n",
    "#     return np.array(extracted_features)\n",
    "\n",
    "# # Example usage:\n",
    "# train_extracted_features = extract_features(X_train_encoded)\n",
    "# test_extracted_features = extract_features(X_test_encoded)\n",
    "# len(test_extracted_features)\n",
    "\n",
    "####################### Susbset selection\n",
    "############# not working\n",
    "# def select_common_features(train_features, test_features):\n",
    "#     common_features = set(train_features[0]).intersection(set(test_features[0]))\n",
    "#     train_selected_features = []\n",
    "#     test_selected_features = []\n",
    "#     for train_feature, test_feature in zip(train_features, test_features):\n",
    "#         selected_train_feature = [value for value in train_feature if value in common_features]\n",
    "#         selected_test_feature = [value for value in test_feature if value in common_features]\n",
    "#         if selected_train_feature:\n",
    "#             train_selected_features.append(selected_train_feature)\n",
    "#         else:\n",
    "#             train_selected_features.append([])\n",
    "#         if selected_test_feature:\n",
    "#             test_selected_features.append(selected_test_feature)\n",
    "#         else:\n",
    "#             test_selected_features.append([])\n",
    "#     return np.array(train_selected_features), np.array(test_selected_features)\n",
    "\n",
    "# # Example usage:\n",
    "# try:\n",
    "#     train_selected_features, test_selected_features = select_common_features(x_train, x_test)\n",
    "# except KeyError:\n",
    "#     print(\"gg brother\")\n",
    "# len(x_test)\n",
    "# len(x_train)\n",
    "# train_selected_features, test_selected_features = select_common_features(X_train_encoded, X_test_encoded)\n",
    "#################### padding last choice i guess\n",
    "# import numpy as np\n",
    "\n",
    "# def pad_features(features, max_length):\n",
    "#     padded_features = []\n",
    "#     for feature in features:\n",
    "#         if len(feature) < max_length:\n",
    "#             padded_feature = np.pad(feature, (0, max_length - len(feature)), 'constant')\n",
    "#         else:\n",
    "#             padded_feature = feature[:max_length]\n",
    "#         padded_features.append(padded_feature)\n",
    "#     return np.array(padded_features)\n",
    "\n",
    "# # Example usage:\n",
    "# max_length = 314  # Maximum length of feature vectors\n",
    "# padded_train_features = pad_features(x_train, max_length)\n",
    "# padded_test_features = pad_features(x_test, max_length)\n",
    "# len(padded_train_features),len(padded_test_features)\n",
    "# padded_test_features\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_enco = LabelEncoder()\n",
    "transformer = CT([(\"one_hot\",one_hot,feature_not_numbers)],\n",
    "                remainder=\"passthrough\")\n",
    "X_train_encoded = transformer.fit_transform(missing_val_filled_train_set_df)\n",
    "# X_test_encoded = transformer.fit_transform(transforemd_test_Set_df)\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train_encoded, transforemd_test_Set_df, test_size=0.4, random_state=42)\n",
    "X_train_split.shape,X_val_split.shape, y_train_split.shape, y_val_split.shape\n",
    "# y_train_encoded = one_hot.fit_transform(ytrain_set_df_target)\n",
    "# x_train_array = X_train_encoded.toarray().astype(np.float32)\n",
    "# # y_train_array = y_train.toarray().astype(np.float32)\n",
    "# x_test_array = X_test_encoded.toarray().astype(np.float32)\n",
    "# y_train_array = np.array(ytransformed_target_train_set_df)\n",
    "\n",
    "model.fit(X_train_split,y_train_split)\n",
    "y_test_preds = model.predict(transforemd_test_Set_df)\n",
    "len(X_train_encoded)\n",
    "# model.score(X_test_encoded,y_test)\n",
    "# for i in range(len(x_train_array)):\n",
    "#     if x_test_array[i] == x_train_array[i]:\n",
    "#         print(True)\n",
    "#     else:\n",
    "#         print(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "503202c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 740 features, but RandomForestClassifier is expecting 1138 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train_split,y_train_split)\n\u001b[1;32m----> 2\u001b[0m y_test_preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test_encoded)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y_test_preds))\n",
      "File \u001b[1;32m~\\desktop\\MLDL projects\\env\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:820\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    799\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    800\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;124;03m    Predict class for X.\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;124;03m        The predicted classes.\u001b[39;00m\n\u001b[0;32m    819\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 820\u001b[0m     proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_proba(X)\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    823\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39margmax(proba, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\desktop\\MLDL projects\\env\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:862\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    860\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    861\u001b[0m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[1;32m--> 862\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_X_predict(X)\n\u001b[0;32m    864\u001b[0m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[0;32m    865\u001b[0m n_jobs, _, _ \u001b[38;5;241m=\u001b[39m _partition_estimators(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n",
      "File \u001b[1;32m~\\desktop\\MLDL projects\\env\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:602\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;124;03mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001b[39;00m\n\u001b[0;32m    601\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 602\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, dtype\u001b[38;5;241m=\u001b[39mDTYPE, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc):\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\desktop\\MLDL projects\\env\\Lib\\site-packages\\sklearn\\base.py:588\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    585\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 588\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[0;32m    590\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\desktop\\MLDL projects\\env\\Lib\\site-packages\\sklearn\\base.py:389\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 389\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    390\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    391\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    392\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 740 features, but RandomForestClassifier is expecting 1138 features as input."
     ]
    }
   ],
   "source": [
    "model.fit(X_train_split,y_train_split)\n",
    "y_test_preds = model.predict(X_test_encoded)\n",
    "print(len(y_test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261f0dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_train[1:368])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1358febc",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_val_filled_train_set_df\n",
    "ytrain_set_df_target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed8d689",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18943cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "# Assuming you have a DataFrame with one-hot encoded features and target variable\n",
    "missing_val_filled_train_set_df['CoapplicantIncome'] = missing_val_filled_train_set_df['CoapplicantIncome'].astype(str)\n",
    "missing_val_filled_train_set_df['LoanAmount'] = missing_val_filled_train_set_df['LoanAmount'].astype(str)\n",
    "missing_val_filled_train_set_df['Loan_Amount_Term'] = missing_val_filled_train_set_df['Loan_Amount_Term'].astype(str)\n",
    "missing_val_filled_train_set_df['Credit_History'] = missing_val_filled_train_set_df['Credit_History'].astype(str)\n",
    "X_train = missing_val_filled_train_set_df\n",
    "y_train = ytrain_set_df_target\n",
    "# Create an instance of the OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Fit and transform the input features\n",
    "X_train_encoded = encoder.fit_transform(X_train)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train_encoded, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an instance of the classification algorithm\n",
    "# we will be using all three models at once\n",
    "\n",
    "model_dict = {'LinearSVC':LinearSVC(),\n",
    "              'KNeighborsClassifier':KNeighborsClassifier(),\n",
    "              'SVC':SVC()}\n",
    "# now we will create a model that will fit and score on these models\n",
    "def fit_and_Score(model, X_train_split, y_train_split, X_val_split, y_val_split):\n",
    "    model_scores_list = []\n",
    "    for key,classifier in model_dict.items():\n",
    "        model_fit = classifier.fit(X_train_split,y_train_split)\n",
    "        model_scores = model_fit.score(X_val_split,y_val_split)\n",
    "        model_scores_list.append(model_scores)\n",
    "    return model_scores_list\n",
    "\n",
    "for classifier in model_dict:\n",
    "    function_call = fit_and_Score(model = classifier,\n",
    "                                  X_train_split = X_train_split,\n",
    "                                  y_train_split = y_train_split,\n",
    "                                  X_val_split = X_val_split,\n",
    "                                  y_val_split = y_val_split)\n",
    "Final_score = function_call\n",
    "Final_score\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "# # Evaluate the model\n",
    "# accuracy = classifier.score(X_val_split, y_val_split)\n",
    "# accuracy\n",
    "# X_train_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae51cbe",
   "metadata": {},
   "source": [
    "## Now we will do cross validation of the above data, to improve the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b292efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {}\n",
    "for index,i in enumerate(Final_score):\n",
    "    result_dict.update({list(model_dict)[index]:i})\n",
    "result_dict\n",
    "################################## starting cvs\n",
    "from sklearn.model_selection import cross_val_score\n",
    "np.random.seed(42)\n",
    "# acc = cross_val_score(clf, X, y, cv=5, scoring=\"accuracy\")\n",
    "# acc_score = np.mean(acc)\n",
    "cvs_result_list = []\n",
    "for key,classifier in model_dict.items():\n",
    "    MODEL = classifier\n",
    "    scoring_acc = cross_val_score(MODEL,X_train_encoded,y_train,cv = 5, scoring=\"accuracy\")\n",
    "    cross_validated_Score = np.mean(scoring_acc)\n",
    "    cvs_result_list.append(cross_validated_Score)\n",
    "cvs_result_list\n",
    "y_preds = MODEL.predict_proba(X_test_encoded).reshape(-1,)\n",
    "# test_probs = model.predict_proba(test_features).reshape(-1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce31f5b",
   "metadata": {},
   "source": [
    "## checking various metric results like classification report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2ba8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### trying to use the fitted model on a different test dataset of different legnth as compared to the train datatset\n",
    "\n",
    "# encoder = OneHotEncoder()\n",
    "# np.random.seed(42)\n",
    "# X_train_split = encoder.fit_transform(x_train)\n",
    "# # y_train_split = encoder.fit_transform(y_train)\n",
    "# y_train_split = ytrain_set_df_target\n",
    "# X_test_split = encoder.fit_transform(missing_val_filled_test_set_df)\n",
    "# clf = KNeighborsClassifier()\n",
    "# clf = TruncatedSVD(100)\n",
    "# Xpca = clf.fit_transform(X)\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# def apply_pca(data, n_components):\n",
    "#     pca = PCA(n_components=n_components)\n",
    "#     reduced_data = pca.fit_transform(data)\n",
    "#     return reduced_data\n",
    "\n",
    "# # Example usage:\n",
    "# n_components = 367 # Number of desired components\n",
    "# train_reduced_features = apply_pca(X_train_split, n_components)\n",
    "# test_reduced_features = apply_pca(X_test_split, n_components)\n",
    "# fittedmodel = clf.fit(train_reduced_features,y_train_split)\n",
    "# # \n",
    "\n",
    "# score = fittedmodel.score(test_reduced_features,y_train_split)\n",
    "# score\n",
    "########################################## trying to make the length of the test and train dataset equal\n",
    "# from sklearn.decomposition import TruncatedSVD\n",
    "# model = KNeighborsClassifier()\n",
    "# encoder = OneHotEncoder()\n",
    "# X_train_split = encoder.fit_transform(x_train)\n",
    "# X_test_split = encoder.fit_transform(missing_val_filled_test_set_df)\n",
    "# # clf = TruncatedSVD(100)\n",
    "# # X2_train_split = clf.fit_transform(X_train_split)\n",
    "# # X2_test_split = clf.fit_transform(X_test_split)\n",
    "\n",
    "# # from sklearn.decomposition import PCA\n",
    "\n",
    "# # def apply_pca(data, n_components):\n",
    "# #     pca = PCA(n_components=n_components)\n",
    "# #     reduced_data = pca.fit_transform(data)\n",
    "# #     return reduced_data\n",
    "\n",
    "# # # Example usage:\n",
    "# # n_components = 10 # Number of desired components\n",
    "# # train_reduced_features = apply_pca(X2_train_split, n_components)\n",
    "# # test_reduced_features = apply_pca(X2_test_split, n_components)\n",
    "# fittedmodel = model.fit(X_train_split,y_train)\n",
    "# # \n",
    "# y_preds_test = fittedmodel.predict(X_test_split)\n",
    "# # len(train_reduced_features),len(test_reduced_features)\n",
    "# score = fittedmodel.score(X_test_split,y_preds_test)\n",
    "# score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed097385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(X_train_encoded,y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034194e8",
   "metadata": {},
   "source": [
    "### pos label important info\n",
    " if you have a binary classification problem where you are predicting whether an email is spam (positive class) or not spam (negative class), you can set the \"pos_label\" to 1 (spam) to calculate accuracy specifically for spam emails. This helps you understand how well your model performs in identifying spam emails while ignoring its performance on non-spam emails.\n",
    "\n",
    "It's important to note that the \"pos_label\" parameter is not universally present in all accuracy calculation functions or libraries. Its availability may depend on the specific programming language or machine learning framework you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6205d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing all the basic tools we need\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "## importing all the ml models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "## importing preprocessing tools\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "## Model Evaluating tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n",
    "from sklearn.metrics import ConfusionMatrixDisplay,confusion_matrix,classification_report\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "## setting random seed\n",
    "np.random.seed(42)\n",
    "dataset = train_set\n",
    "##experimentation with our dataset to know more about it\n",
    "\n",
    "# x = dataset[\"target\"].value_counts()\n",
    "\n",
    "## splitting our data into train and test sets\n",
    "x_train = missing_val_filled_train_set_df\n",
    "y_train2 = ytrain_set_df_target\n",
    "X_transformed = one_hot.fit_transform(x_train)\n",
    "\n",
    "X = X_transformed\n",
    "y = y_train2\n",
    "\n",
    "x_test = missing_val_filled_test_set_df\n",
    "## preprocrssing our data and handling the missing values\n",
    "one_hot = OneHotEncoder()\n",
    "\n",
    "## now we will be creating a function to use these models again and again \n",
    "# model_dict = {'LinearSVC':LinearSVC(),\n",
    "#               \"LogisticRegression\":LogisticRegression(),\n",
    "#               'KNeighborsClassifier':KNeighborsClassifier(),\n",
    "#               'SVC':SVC(),\n",
    "#               \"RandomForestClassifier\":RandomForestClassifier()}\n",
    "model_dict = {'LinearSVC':LinearSVC(),\n",
    "              'KNeighborsClassifier':KNeighborsClassifier(),\n",
    "              'SVC':SVC()\n",
    "              }\n",
    "accuracy_dict = {}\n",
    "precision_dict = {}\n",
    "recall_dict = {}\n",
    "scoring_metrics = [\"accuracy\",\"precision\",\"recall\"]\n",
    "def model_function(model,X_test,X_train,y_test,y_train):\n",
    "    for key,classifier in model_dict.items():\n",
    "        model_fit = classifier.fit(X_train,y_train)\n",
    "        model_score = model_fit.score(X_test,y_test)\n",
    "## Scoring using cross validation to get better results on accuracy,precision,recall use the following code\n",
    "## instead of the code not commented\n",
    "#     np.random.seed(42)\n",
    "#     scoring_metrics = [\"accuracy\",\"precision\",\"recall\"]\n",
    "#     for key,classifier in model_dict.items():\n",
    "#         model_fit = classifier.fit(X_train,y_train)\n",
    "#         for metric in scoring_metrics:\n",
    "#             model_cvs = cross_val_score(classifier,X,y,cv=5,scoring=metric)\n",
    "#             model_score = np.mean(model_cvs)\n",
    "#             print(f'Model:{key} -- {metric} Score = {model_score}')\n",
    "        accuracy_dict[key] = model_score\n",
    "        y_preds = classifier.predict(X_test)\n",
    "        \n",
    "        \n",
    "        \n",
    "        precision_dict[key] = precision_score(y_true=y_test, y_pred=y_preds, pos_label=\"Y\")\n",
    "        recall_dict[key] = recall_score(y_true=y_test, y_pred=y_preds, pos_label=\"Y\")\n",
    "        model_compare_acc = pd.DataFrame(accuracy_dict,index = [\"accuracy\"])\n",
    "        model_compare_pre = pd.DataFrame(precision_dict,index = [\"precision\"])       \n",
    "        model_compare_rec = pd.DataFrame(recall_dict,index = [\"recall_Score\"])\n",
    "\n",
    "    yield model_compare_acc\n",
    "    yield model_compare_pre\n",
    "    yield model_compare_rec\n",
    "    ## keeping extra yields in case if we require more models to test\n",
    "    yield \"\"\n",
    "    yield \"\"\n",
    "    yield \"\"\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "for classifier in model_dict:\n",
    "    method_call = model_function(model = classifier,\n",
    "                                 X_test = X_test,\n",
    "                                 X_train = X_train,\n",
    "                                 y_test = y_test,\n",
    "                                 y_train = y_train)\n",
    "\n",
    "for lim in range(len(model_dict)):\n",
    "    print(method_call.__next__())\n",
    "# plotting our results\n",
    "# use the below commented code in case if there is error in plotting graphs\n",
    "model_compare_acc = pd.DataFrame(accuracy_dict,index = [\"accuracy\"])\n",
    "model_compare_pre = pd.DataFrame(precision_dict,index = [\"precision\"])       \n",
    "model_compare_rec = pd.DataFrame(recall_dict,index = [\"recall_Score\"])\n",
    "print(\"This Result is on a single test set without using corss validation\")\n",
    "##### Use the below code to print al the metrics ourside of the function\n",
    "# print(f'{model_compare_acc}\\n {model_compare_pre}\\n {model_compare_rec}')\n",
    "acc_plot = model_compare_acc.T.plot.bar()\n",
    "pre_plot = model_compare_pre.T.plot.bar()\n",
    "recall_plot = model_compare_rec.T.plot.bar()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880288ce",
   "metadata": {},
   "source": [
    "## Result Analysis:\n",
    "from the above result we can see that models, Logistic Regression, SVC, and RFC are giving the same results hence we will be discarding them also SVC is giving better results as compared to LinearSVC and KneighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a91fa9",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "to improve SVC model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cb6eb7",
   "metadata": {},
   "source": [
    "## 1) cross validation for svc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2953f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "svc = SVC()\n",
    "fittedmodel = svc.fit(X,y)\n",
    "scoring_metrics = [\"accuracy\",\"roc_auc\",\"f1\"]\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# # Restore warnings\n",
    "# warnings.filterwarnings(\"default\")\n",
    "for metric in scoring_metrics:\n",
    "    cvs = cross_val_score(svc, X, y, cv=5, scoring=metric, verbose=1)\n",
    "    cross_validated_score = np.mean(cvs)\n",
    "    print(f'Model:SVC -- {metric} Score = {cross_validated_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfec3b8",
   "metadata": {},
   "source": [
    "## 2) Random Search Cross Vaildation(RSCV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7626b690",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameter tuning using RSCV\n",
    "# Creating a HP grid for SVC\n",
    "np.random.seed(42)\n",
    "SVC_grid = {\"C\":np.logspace(-4, 4 ,20),\n",
    "            \"break_ties\":[5,5,10]}\n",
    "rs_svc = RandomizedSearchCV(SVC(),\n",
    "                           param_distributions=SVC_grid,\n",
    "                          cv=5,\n",
    "                          n_iter=20,\n",
    "                          verbose=True)\n",
    "rs_svc.fit(X_train,y_train)\n",
    "rs_svc.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee685de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### using these params in above model fitting\n",
    "import warnings\n",
    "svc = SVC(break_ties=5, C=1.623776739188721)\n",
    "fittedmodel = svc.fit(X,y)\n",
    "scoring_metrics = [\"accuracy\",\"roc_auc\",\"f1\"]\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# # Restore warnings\n",
    "# warnings.filterwarnings(\"default\")\n",
    "for metric in scoring_metrics:\n",
    "    cvs = cross_val_score(svc, X, y, cv=5, scoring=metric, verbose=1)\n",
    "    cross_validated_score = np.mean(cvs)\n",
    "    print(f'Model:SVC -- {metric} Score = {cross_validated_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a119c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6d8af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "classifier = SVC()\n",
    "model_fit = classifier.fit(X_train,y_train)\n",
    "\n",
    "y_preds = classifier.predict(X_test)  \n",
    "\n",
    "# ## Plot ROC Curve and finding the AUC metric\n",
    "# # AUC metric is generally written below the graph\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# # Compute ROC curve\n",
    "# fpr, tpr, thresholds = roc_auc_score(y_test, y_preds)\n",
    "\n",
    "# # Plot ROC curve\n",
    "# plt.plot(fpr, tpr)\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('ROC Curve')\n",
    "# plt.show()\n",
    "## Confusion matrix\n",
    "# seaborn heat map\n",
    "sns.set(font_scale=1.5) # Increase font size\n",
    " \n",
    "def plot_conf_mat(y_test, y_preds):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix using Seaborn's heatmap().\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(3, 3))\n",
    "    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n",
    "                     annot=True, # Annotate the boxes\n",
    "                     cbar=False)\n",
    "    plt.xlabel(\"Predicted label\") # predictions go on the x-axis\n",
    "    plt.ylabel(\"True label\") # true labels go on the y-axis \n",
    "  \n",
    "plot_conf_mat(y_test, y_preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689faae8",
   "metadata": {},
   "source": [
    "# Approaching the problem using regression\n",
    "on Loan_Amount vs other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32569b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We already got the tranformed and missing values filled dataset hence we would just start training our model\n",
    "X_reg = X\n",
    "y_reg = missing_val_filled_train_set_df[\"LoanAmount\"]\n",
    "y_reg_transformed = label_enco.fit_transform(y_reg)\n",
    "np.random.seed(42)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "model_dcit_reg = {\"RandomForestRegressor\": RandomForestRegressor(),\n",
    "                  \"Ridge\": Ridge(),\n",
    "                  \"Lasso\": Lasso()\n",
    "                 }\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_reg,y_reg_transformed,test_size=0.2)\n",
    "\n",
    "for key,model in model_dcit_reg.items():\n",
    "    fitted_model = model.fit(X_train,y_train)\n",
    "    score = fitted_model.score(X_test,y_test)\n",
    "    print(f'{key} score = {score}')\n",
    "    \n",
    "#Lasso Regression\n",
    "\n",
    "\n",
    "\n",
    "# #Initializing the Lasso Regressor with Normalization Factor as True\n",
    "# lasso_reg = Lasso(normalize=True)\n",
    "# #Fitting the Training data to the Lasso regressor\n",
    "# lasso_reg.fit(X_train,Y_train)\n",
    "# #Predicting for X_test\n",
    "# y_pred_lass =lasso_reg.predict(X_test)\n",
    "# #Printing the Score with RMLSE\n",
    "# print(\"\\n\\nLasso SCORE : \", score(y_pred_lass, actual_cost))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ea7587",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d05c98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "np.random.seed(42)\n",
    "scoring_metrics_reg  = [\"r2\"]\n",
    "for key,model in model_dcit_reg.items():\n",
    "    fitted_model = model.fit(X_train,y_train)\n",
    "    for metric in scoring_metrics_reg:\n",
    "        score = cross_val_score(fitted_model, X_reg, y_reg_transformed, cv=5, scoring=metric)\n",
    "        resulted_score = np.mean(score)\n",
    "        print(f'{key}:{metric} = {resulted_score}')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1e8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import get_scorer_names\n",
    "get_scorer_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea47f4be",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f4e2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### will try to find the best parameters for Random forest Regressor and Ridge Regressor to improve our accuracy\n",
    "## Hyperparameter tuning using RSCV\n",
    "# Creating a HP grid for RFR \n",
    "# np.random.seed(42)\n",
    "# RFR_grid = {\n",
    "#             'bootstrap': [True, False],\n",
    "#             'max_depth': [20, 40, 60, 80, 100, None],\n",
    "#             'max_features': ['auto'],\n",
    "#             'min_samples_leaf': [1, 2, 4],\n",
    "#             'min_samples_split': [2, 5, 10],\n",
    "#             'n_estimators': [100,200]}\n",
    "# rs_rfr = RandomizedSearchCV(RandomForestRegressor(),\n",
    "#                             param_distributions=RFR_grid,\n",
    "#                             cv=5,\n",
    "#                             n_iter=20,\n",
    "#                             verbose=True)\n",
    "# rs_rfr.fit(X_train,y_train)\n",
    "\n",
    "# rs_rfr.best_params_\n",
    "############ for Lasso reg\n",
    "Lasso_grid = {\n",
    "            'alpha':[0.5,1],\n",
    "    'fit_intercept':[True,False],\n",
    "    'precompute':[True,False],\n",
    "    'copy_X':[True,False],\n",
    "    'max_iter':[50,100,200]\n",
    "        }\n",
    "rs_lasso = RandomizedSearchCV(Lasso(),\n",
    "                            param_distributions=Lasso_grid,\n",
    "                            cv=5,\n",
    "                            n_iter=20,\n",
    "                            verbose=True)\n",
    "rs_lasso.fit(X_train,y_train)\n",
    "\n",
    "rs_lasso.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a56d7a2",
   "metadata": {},
   "source": [
    "#### These are the best parameters that we got  for HP tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f260e761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "np.random.seed(42)\n",
    "model_dict_reg =  {\"RandomForestRegressor\": RandomForestRegressor(n_estimators=500,\n",
    "                                                                  min_samples_leaf=2,\n",
    "                                                                  min_samples_split=5,\n",
    "                                                                  max_features='auto',\n",
    "                                                                  bootstrap=True,\n",
    "                                                                  max_depth=20),\n",
    "                   \"Lasso\": Lasso(precompute=True,\n",
    "                                 max_iter=50,\n",
    "                                 fit_intercept=True,\n",
    "                                 copy_X=False,\n",
    "                                 alpha=0.5)}\n",
    "scoring_metrics_reg  = [\"r2\"]\n",
    "for key,model in model_dcit_reg.items():\n",
    "    fitted_model = model.fit(X_train,y_train)\n",
    "    for metric in scoring_metrics_reg:\n",
    "        score = cross_val_score(fitted_model, X_reg, y_reg_transformed, cv=15, scoring=metric)\n",
    "        resulted_score = np.mean(score)\n",
    "        print(f'{key}:{metric} = {resulted_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e8bb2e",
   "metadata": {},
   "source": [
    "#### Hence these are our final regression r2 scores for Loan Eligibilty predicter problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5753e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
